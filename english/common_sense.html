<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Common sense</title>
    <meta name="description" content="">
    <meta name="generator" content="VuePress 1.3.0">
    
    
    <link rel="preload" href="/static/dataset/assets/css/0.styles.ba7cd554.css" as="style"><link rel="preload" href="/static/dataset/assets/js/app.18b431e4.js" as="script"><link rel="preload" href="/static/dataset/assets/js/2.c3866e2d.js" as="script"><link rel="preload" href="/static/dataset/assets/js/3.d90b44d3.js" as="script"><link rel="preload" href="/static/dataset/assets/js/11.e3010e9b.js" as="script"><link rel="prefetch" href="/static/dataset/assets/js/10.f1dac487.js"><link rel="prefetch" href="/static/dataset/assets/js/12.69ab5c7e.js"><link rel="prefetch" href="/static/dataset/assets/js/13.12d77e1f.js"><link rel="prefetch" href="/static/dataset/assets/js/14.60166ccc.js"><link rel="prefetch" href="/static/dataset/assets/js/15.5a1179c8.js"><link rel="prefetch" href="/static/dataset/assets/js/16.719bd668.js"><link rel="prefetch" href="/static/dataset/assets/js/17.d8d9f4bc.js"><link rel="prefetch" href="/static/dataset/assets/js/18.ac6075e9.js"><link rel="prefetch" href="/static/dataset/assets/js/19.ae3fb24f.js"><link rel="prefetch" href="/static/dataset/assets/js/20.fa9d9a13.js"><link rel="prefetch" href="/static/dataset/assets/js/21.0ac7ccdd.js"><link rel="prefetch" href="/static/dataset/assets/js/22.23d94784.js"><link rel="prefetch" href="/static/dataset/assets/js/23.fcfcda6d.js"><link rel="prefetch" href="/static/dataset/assets/js/24.95f1b6c4.js"><link rel="prefetch" href="/static/dataset/assets/js/25.a3db53f4.js"><link rel="prefetch" href="/static/dataset/assets/js/26.5f977712.js"><link rel="prefetch" href="/static/dataset/assets/js/27.fb650c13.js"><link rel="prefetch" href="/static/dataset/assets/js/28.d852726c.js"><link rel="prefetch" href="/static/dataset/assets/js/29.3aea91f5.js"><link rel="prefetch" href="/static/dataset/assets/js/30.9bfdd5d3.js"><link rel="prefetch" href="/static/dataset/assets/js/31.d88f66c4.js"><link rel="prefetch" href="/static/dataset/assets/js/32.6db44253.js"><link rel="prefetch" href="/static/dataset/assets/js/33.4923356c.js"><link rel="prefetch" href="/static/dataset/assets/js/34.a09bf3e8.js"><link rel="prefetch" href="/static/dataset/assets/js/35.ab3da096.js"><link rel="prefetch" href="/static/dataset/assets/js/36.6fb81f76.js"><link rel="prefetch" href="/static/dataset/assets/js/37.94ca99d1.js"><link rel="prefetch" href="/static/dataset/assets/js/38.d9c2f226.js"><link rel="prefetch" href="/static/dataset/assets/js/39.b4c97846.js"><link rel="prefetch" href="/static/dataset/assets/js/4.93f0fe50.js"><link rel="prefetch" href="/static/dataset/assets/js/40.9228dff7.js"><link rel="prefetch" href="/static/dataset/assets/js/41.2b8f70f5.js"><link rel="prefetch" href="/static/dataset/assets/js/42.bef5e921.js"><link rel="prefetch" href="/static/dataset/assets/js/43.ff9a4086.js"><link rel="prefetch" href="/static/dataset/assets/js/44.d0e8035a.js"><link rel="prefetch" href="/static/dataset/assets/js/45.a9d46bfa.js"><link rel="prefetch" href="/static/dataset/assets/js/46.e667b4c8.js"><link rel="prefetch" href="/static/dataset/assets/js/47.4801c9b1.js"><link rel="prefetch" href="/static/dataset/assets/js/48.80551b37.js"><link rel="prefetch" href="/static/dataset/assets/js/49.f5ed72ba.js"><link rel="prefetch" href="/static/dataset/assets/js/5.2eca2d13.js"><link rel="prefetch" href="/static/dataset/assets/js/50.1122ca28.js"><link rel="prefetch" href="/static/dataset/assets/js/51.9e4544d9.js"><link rel="prefetch" href="/static/dataset/assets/js/6.da4b743b.js"><link rel="prefetch" href="/static/dataset/assets/js/7.bb585808.js"><link rel="prefetch" href="/static/dataset/assets/js/8.866c325e.js"><link rel="prefetch" href="/static/dataset/assets/js/9.a351829b.js">
    <link rel="stylesheet" href="/static/dataset/assets/css/0.styles.ba7cd554.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div id="global-layout" data-v-1cbc1c23><header class="bk-dark" data-v-1cbc1c23><video autoplay="autoplay" loop="loop" muted="muted" data-v-1cbc1c23><source src="/static/dataset/assets/media/bk.417d52db.mp4" type="video/mp4" data-v-1cbc1c23></video> <div data-v-1cbc1c23><div class="header-content" data-v-1cbc1c23><h1 data-v-1cbc1c23>NLP-PROGRESS</h1> <h2 data-v-1cbc1c23>Repository to trasck the progress in Natural Language Processing (NLP), including the datasets and the current state-of-the-art for the most common NLP tasks.</h2> <a href="#" class="btn" data-v-1cbc1c23><i class="iconfont icon-github" data-v-1cbc1c23></i>
                    View on GitHub
                </a></div></div></header> <div class="theme-container no-navbar" data-v-1cbc1c23><!----> <div class="sidebar-mask"></div> <div class="sidebar-wrap sidebar-wrap-absolute"><!----></div> <main class="page"> <div class="theme-default-content content__default"><h1 id="common-sense"><a href="#common-sense" class="header-anchor">#</a> Common sense</h1> <p>Common sense reasoning tasks are intended to require the model to go beyond pattern
recognition. Instead, the model should use &quot;common sense&quot; or world knowledge
to make inferences.</p> <h3 id="event2mind"><a href="#event2mind" class="header-anchor">#</a> Event2Mind</h3> <p>Event2Mind is a crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations.
Given an event described in a short free-form text, a model should reason about the likely intents and reactions of the
event's participants. Models are evaluated based on average cross-entropy (lower is better).</p> <table><thead><tr><th>Model</th> <th style="text-align:center;">Dev</th> <th style="text-align:center;">Test</th> <th>Paper / Source</th> <th>Code</th></tr></thead> <tbody><tr><td>BiRNN 100d (Rashkin et al., 2018)</td> <td style="text-align:center;">4.25</td> <td style="text-align:center;">4.22</td> <td><a href="https://arxiv.org/abs/1805.06939" target="_blank" rel="noopener noreferrer">Event2Mind: Commonsense Inference on Events, Intents, and Reactions<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr> <tr><td>ConvNet (Rashkin et al., 2018)</td> <td style="text-align:center;">4.44</td> <td style="text-align:center;">4.40</td> <td><a href="https://arxiv.org/abs/1805.06939" target="_blank" rel="noopener noreferrer">Event2Mind: Commonsense Inference on Events, Intents, and Reactions<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr></tbody></table> <h3 id="swag"><a href="#swag" class="header-anchor">#</a> SWAG</h3> <p>Situations with Adversarial Generations (SWAG) is a dataset consisting of 113k multiple
choice questions about a rich spectrum of grounded situations.</p> <table><thead><tr><th>Model</th> <th style="text-align:center;">Dev</th> <th style="text-align:center;">Test</th> <th>Paper / Source</th> <th>Code</th></tr></thead> <tbody><tr><td>BERT Large (Devlin et al., 2018)</td> <td style="text-align:center;">86.6</td> <td style="text-align:center;">86.3</td> <td><a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener noreferrer">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr> <tr><td>BERT Base (Devlin et al., 2018)</td> <td style="text-align:center;">81.6</td> <td style="text-align:center;">-</td> <td><a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener noreferrer">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr> <tr><td>ESIM + ELMo (Zellers et al., 2018)</td> <td style="text-align:center;">59.1</td> <td style="text-align:center;">59.2</td> <td><a href="http://arxiv.org/abs/1808.05326" target="_blank" rel="noopener noreferrer">SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr> <tr><td>ESIM + GloVe (Zellers et al., 2018)</td> <td style="text-align:center;">51.9</td> <td style="text-align:center;">52.7</td> <td><a href="http://arxiv.org/abs/1808.05326" target="_blank" rel="noopener noreferrer">SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr></tbody></table> <h3 id="winograd-schema-challenge"><a href="#winograd-schema-challenge" class="header-anchor">#</a> Winograd Schema Challenge</h3> <p>The <a href="https://www.aaai.org/ocs/index.php/KR/KR12/paper/view/4492" target="_blank" rel="noopener noreferrer">Winograd Schema Challenge<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>
is a dataset for common sense reasoning. It employs Winograd Schema questions that
require the resolution of anaphora: the system must identify the antecedent of an ambiguous pronoun in a statement. Models
are evaluated based on accuracy.</p> <p>Example:</p> <p>The trophy doesn’t fit in the suitcase because <em>it</em> is too big. What is too big?
Answer 0: the trophy. Answer 1: the suitcase</p> <table><thead><tr><th>Model</th> <th style="text-align:center;">Score</th> <th>Paper / Source</th> <th>Code</th></tr></thead> <tbody><tr><td>Word-LM-partial (Trinh and Le, 2018)</td> <td style="text-align:center;">62.6</td> <td><a href="https://arxiv.org/abs/1806.02847" target="_blank" rel="noopener noreferrer">A Simple Method for Commonsense Reasoning<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr> <tr><td>Char-LM-partial (Trinh and Le, 2018)</td> <td style="text-align:center;">57.9</td> <td><a href="https://arxiv.org/abs/1806.02847" target="_blank" rel="noopener noreferrer">A Simple Method for Commonsense Reasoning<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr> <tr><td>USSM + Supervised DeepNet + KB (Liu et al., 2017)</td> <td style="text-align:center;">52.8</td> <td><a href="https://aaai.org/ocs/index.php/SSS/SSS17/paper/view/15392" target="_blank" rel="noopener noreferrer">Combing Context and Commonsense Knowledge Through Neural Networks for Solving Winograd Schema Problems<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr></tbody></table> <h3 id="winograd-nli-wnli"><a href="#winograd-nli-wnli" class="header-anchor">#</a> Winograd NLI (WNLI)</h3> <p>WNLI is a relaxation of the Winograd Schema Challenge proposed as part of the <a href="https://arxiv.org/abs/1804.07461" target="_blank" rel="noopener noreferrer">GLUE benchmark<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> and a conversion to the natural language inference (NLI) format. The task is to predict if the sentence with the pronoun substituted is entailed by the original sentence. While the training set is balanced between two classes (entailment and not entailment), the test set is imbalanced between them (35% entailment, 65% not entailment). The majority baseline is thus 65%, while for the Winograd Schema Challenge it is 50% (<a href="https://www.aaai.org/ocs/index.php/SSS/SSS17/paper/view/15392" target="_blank" rel="noopener noreferrer">Liu et al., 2017<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>). The latter is more challenging.</p> <p>Results are available at the <a href="https://gluebenchmark.com/leaderboard" target="_blank" rel="noopener noreferrer">GLUE leaderboard<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>. Here is a subset of results of recent models:</p> <table><thead><tr><th>Model</th> <th style="text-align:center;">Score</th> <th>Paper / Source</th> <th>Code</th></tr></thead> <tbody><tr><td>XLNet-Large (ensemble) (Yang et al., 2019)</td> <td style="text-align:center;">90.4</td> <td><a href="https://arxiv.org/pdf/1906.08237.pdf" target="_blank" rel="noopener noreferrer">XLNet: Generalized Autoregressive Pretraining for Language Understanding<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td><a href="https://github.com/zihangdai/xlnet/" target="_blank" rel="noopener noreferrer">Official<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>MT-DNN-ensemble (Liu et al., 2019)</td> <td style="text-align:center;">89.0</td> <td><a href="https://arxiv.org/pdf/1904.09482.pdf" target="_blank" rel="noopener noreferrer">Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td><a href="https://github.com/namisan/mt-dnn/" target="_blank" rel="noopener noreferrer">Official<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>Snorkel MeTaL(ensemble) (Ratner et al., 2018)</td> <td style="text-align:center;">65.1</td> <td><a href="https://arxiv.org/pdf/1810.02840.pdf" target="_blank" rel="noopener noreferrer">Training Complex Models with Multi-Task Weak Supervision<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td><a href="https://github.com/HazyResearch/metal" target="_blank" rel="noopener noreferrer">Official<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr></tbody></table> <h3 id="visual-common-sense"><a href="#visual-common-sense" class="header-anchor">#</a> Visual Common Sense</h3> <p>Visual Commonsense Reasoning (VCR) is a new task and large-scale dataset for cognition-level visual understanding.
With one glance at an image, we can effortlessly imagine the world beyond the pixels (e.g. that [person1] ordered
pancakes). While this task is easy for humans, it is tremendously difficult for today's vision systems, requiring
higher-order cognition and commonsense reasoning about the world. We formalize this task as Visual Commonsense
Reasoning. In addition to answering challenging visual questions expressed in natural language, a model must provide a
rationale explaining why its answer is true.</p> <table><thead><tr><th>Model</th> <th style="text-align:center;">Q-&gt;A</th> <th style="text-align:center;">QA-&gt;R</th> <th style="text-align:center;">Q-&gt;AR</th> <th>Paper / Source</th> <th>Code</th></tr></thead> <tbody><tr><td>Human Performance University of Washington (Zellers et al. '18)</td> <td style="text-align:center;">91.0</td> <td style="text-align:center;">93.0</td> <td style="text-align:center;">85.0</td> <td><a href="https://arxiv.org/abs/1811.10830" target="_blank" rel="noopener noreferrer">From Recognition to Cognition: Visual Commonsense Reasoning<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr> <tr><td>Recognition to Cognition Networks University of Washington</td> <td style="text-align:center;">65.1</td> <td style="text-align:center;">67.3</td> <td style="text-align:center;">44.0</td> <td><a href="https://arxiv.org/abs/1811.10830" target="_blank" rel="noopener noreferrer">From Recognition to Cognition: Visual Commonsense Reasoning<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td>https://github.com/rowanz/r2c</td></tr> <tr><td>BERT-Base Google AI Language (experiment by Rowan)</td> <td style="text-align:center;">53.9</td> <td style="text-align:center;">64.5</td> <td style="text-align:center;">35.0</td> <td></td> <td>https://github.com/google-research/bert</td></tr> <tr><td>MLB Seoul National University (experiment by Rowan)</td> <td style="text-align:center;">46.2</td> <td style="text-align:center;">36.8</td> <td style="text-align:center;">17.2</td> <td></td> <td>https://github.com/jnhwkim/MulLowBiVQA</td></tr> <tr><td>Random Performance</td> <td style="text-align:center;">25.0</td> <td style="text-align:center;">25.0</td> <td style="text-align:center;">6.2</td> <td></td> <td></td></tr></tbody></table> <h3 id="record"><a href="#record" class="header-anchor">#</a> ReCoRD</h3> <p>Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) is a large-scale reading comprehension dataset which requires commonsense reasoning. ReCoRD consists of queries automatically generated from CNN/Daily Mail news articles; the answer to each query is a text span from a summarizing passage of the corresponding news. The goal of ReCoRD is to evaluate a machine's ability of commonsense reasoning in reading comprehension. ReCoRD is pronounced as [ˈrɛkərd] and is part of the <a href="https://arxiv.org/pdf/1905.00537.pdf" target="_blank" rel="noopener noreferrer">SuperGLUE benchmark<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <table><thead><tr><th>Model</th> <th>EM</th> <th>F1</th> <th>Paper / Source</th> <th>Code</th></tr></thead> <tbody><tr><td>Human Performance Johns Hopkins University (Zhang et al. '18)</td> <td>91.31</td> <td>91.69</td> <td><a href="https://arxiv.org/pdf/1810.12885.pdf" target="_blank" rel="noopener noreferrer">ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr> <tr><td>RoBERTa (Facebook AI)</td> <td>90.0</td> <td>90.6</td> <td><a href="https://arxiv.org/pdf/1907.11692.pdf" target="_blank" rel="noopener noreferrer">RoBERTa: A Robustly Optimized BERT Pretraining Approach<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td><a href="https://github.com/pytorch/fairseq/tree/master/examples/roberta" target="_blank" rel="noopener noreferrer">Official<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>XLNet + MTL + Verifier (ensemble)</td> <td>83.09</td> <td>83.74</td> <td></td> <td></td></tr> <tr><td>CSRLM (single model)</td> <td>81.78</td> <td>82.58</td> <td></td> <td></td></tr></tbody></table></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div> <footer class="bk-dark" data-v-1cbc1c23><div class="footer-content" data-v-1cbc1c23><p data-v-1cbc1c23>NLP-progress maintained by <a href="https://github.com/sebastianruder" data-v-1cbc1c23>sebastianruder</a></p> <p data-v-1cbc1c23>Published with <a href="https://github.com/sebastianruder" data-v-1cbc1c23>GitHub Pages</a></p></div></footer></div><div class="global-ui"></div></div>
    <script src="/static/dataset/assets/js/app.18b431e4.js" defer></script><script src="/static/dataset/assets/js/2.c3866e2d.js" defer></script><script src="/static/dataset/assets/js/3.d90b44d3.js" defer></script><script src="/static/dataset/assets/js/11.e3010e9b.js" defer></script>
  </body>
</html>
