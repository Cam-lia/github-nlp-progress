<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Question answering</title>
    <meta name="description" content="">
    <meta name="generator" content="VuePress 1.3.0">
    
    
    <link rel="preload" href="/static/dataset/assets/css/0.styles.ba7cd554.css" as="style"><link rel="preload" href="/static/dataset/assets/js/app.18b431e4.js" as="script"><link rel="preload" href="/static/dataset/assets/js/2.c3866e2d.js" as="script"><link rel="preload" href="/static/dataset/assets/js/3.d90b44d3.js" as="script"><link rel="preload" href="/static/dataset/assets/js/30.9bfdd5d3.js" as="script"><link rel="prefetch" href="/static/dataset/assets/js/10.f1dac487.js"><link rel="prefetch" href="/static/dataset/assets/js/11.e3010e9b.js"><link rel="prefetch" href="/static/dataset/assets/js/12.69ab5c7e.js"><link rel="prefetch" href="/static/dataset/assets/js/13.12d77e1f.js"><link rel="prefetch" href="/static/dataset/assets/js/14.60166ccc.js"><link rel="prefetch" href="/static/dataset/assets/js/15.5a1179c8.js"><link rel="prefetch" href="/static/dataset/assets/js/16.719bd668.js"><link rel="prefetch" href="/static/dataset/assets/js/17.d8d9f4bc.js"><link rel="prefetch" href="/static/dataset/assets/js/18.ac6075e9.js"><link rel="prefetch" href="/static/dataset/assets/js/19.ae3fb24f.js"><link rel="prefetch" href="/static/dataset/assets/js/20.fa9d9a13.js"><link rel="prefetch" href="/static/dataset/assets/js/21.0ac7ccdd.js"><link rel="prefetch" href="/static/dataset/assets/js/22.23d94784.js"><link rel="prefetch" href="/static/dataset/assets/js/23.fcfcda6d.js"><link rel="prefetch" href="/static/dataset/assets/js/24.95f1b6c4.js"><link rel="prefetch" href="/static/dataset/assets/js/25.a3db53f4.js"><link rel="prefetch" href="/static/dataset/assets/js/26.5f977712.js"><link rel="prefetch" href="/static/dataset/assets/js/27.fb650c13.js"><link rel="prefetch" href="/static/dataset/assets/js/28.d852726c.js"><link rel="prefetch" href="/static/dataset/assets/js/29.3aea91f5.js"><link rel="prefetch" href="/static/dataset/assets/js/31.d88f66c4.js"><link rel="prefetch" href="/static/dataset/assets/js/32.6db44253.js"><link rel="prefetch" href="/static/dataset/assets/js/33.4923356c.js"><link rel="prefetch" href="/static/dataset/assets/js/34.a09bf3e8.js"><link rel="prefetch" href="/static/dataset/assets/js/35.ab3da096.js"><link rel="prefetch" href="/static/dataset/assets/js/36.6fb81f76.js"><link rel="prefetch" href="/static/dataset/assets/js/37.94ca99d1.js"><link rel="prefetch" href="/static/dataset/assets/js/38.d9c2f226.js"><link rel="prefetch" href="/static/dataset/assets/js/39.b4c97846.js"><link rel="prefetch" href="/static/dataset/assets/js/4.93f0fe50.js"><link rel="prefetch" href="/static/dataset/assets/js/40.9228dff7.js"><link rel="prefetch" href="/static/dataset/assets/js/41.2b8f70f5.js"><link rel="prefetch" href="/static/dataset/assets/js/42.bef5e921.js"><link rel="prefetch" href="/static/dataset/assets/js/43.ff9a4086.js"><link rel="prefetch" href="/static/dataset/assets/js/44.d0e8035a.js"><link rel="prefetch" href="/static/dataset/assets/js/45.a9d46bfa.js"><link rel="prefetch" href="/static/dataset/assets/js/46.e667b4c8.js"><link rel="prefetch" href="/static/dataset/assets/js/47.4801c9b1.js"><link rel="prefetch" href="/static/dataset/assets/js/48.80551b37.js"><link rel="prefetch" href="/static/dataset/assets/js/49.f5ed72ba.js"><link rel="prefetch" href="/static/dataset/assets/js/5.2eca2d13.js"><link rel="prefetch" href="/static/dataset/assets/js/50.1122ca28.js"><link rel="prefetch" href="/static/dataset/assets/js/51.9e4544d9.js"><link rel="prefetch" href="/static/dataset/assets/js/6.da4b743b.js"><link rel="prefetch" href="/static/dataset/assets/js/7.bb585808.js"><link rel="prefetch" href="/static/dataset/assets/js/8.866c325e.js"><link rel="prefetch" href="/static/dataset/assets/js/9.a351829b.js">
    <link rel="stylesheet" href="/static/dataset/assets/css/0.styles.ba7cd554.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div id="global-layout" data-v-1cbc1c23><header class="bk-dark" data-v-1cbc1c23><video autoplay="autoplay" loop="loop" muted="muted" data-v-1cbc1c23><source src="/static/dataset/assets/media/bk.417d52db.mp4" type="video/mp4" data-v-1cbc1c23></video> <div data-v-1cbc1c23><div class="header-content" data-v-1cbc1c23><h1 data-v-1cbc1c23>NLP-PROGRESS</h1> <h2 data-v-1cbc1c23>Repository to trasck the progress in Natural Language Processing (NLP), including the datasets and the current state-of-the-art for the most common NLP tasks.</h2> <a href="#" class="btn" data-v-1cbc1c23><i class="iconfont icon-github" data-v-1cbc1c23></i>
                    View on GitHub
                </a></div></div></header> <div class="theme-container no-navbar" data-v-1cbc1c23><!----> <div class="sidebar-mask"></div> <div class="sidebar-wrap sidebar-wrap-absolute"><aside class="sidebar"><!---->  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>目录</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/static/dataset/english/question_answering.html#reading-comprehension" class="sidebar-link">Reading comprehension</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#clicr" class="sidebar-link">CliCR</a></li><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#cnn-daily-mail" class="sidebar-link">CNN / Daily Mail</a></li><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#codah" class="sidebar-link">CODAH</a></li><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#coqa" class="sidebar-link">CoQA</a></li><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#hotpotqa" class="sidebar-link">HotpotQA</a></li><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#ms-marco" class="sidebar-link">MS MARCO</a></li><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#multirc" class="sidebar-link">MultiRC</a></li><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#newsqa" class="sidebar-link">NewsQA</a></li><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#qangaroo" class="sidebar-link">QAngaroo</a></li><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#quac" class="sidebar-link">QuAC</a></li><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#race" class="sidebar-link">RACE</a></li><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#squad" class="sidebar-link">SQuAD</a></li><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#story-cloze-test" class="sidebar-link">Story Cloze Test</a></li><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#swag" class="sidebar-link">SWAG</a></li><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#recipeqa" class="sidebar-link">RecipeQA</a></li><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#narrativeqa" class="sidebar-link">NarrativeQA</a></li><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#duorc" class="sidebar-link">DuoRC</a></li><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#drop" class="sidebar-link">DROP</a></li><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#cosmos-qa" class="sidebar-link">Cosmos QA</a></li></ul></li><li><a href="/static/dataset/english/question_answering.html#open-domain-question-answering" class="sidebar-link">Open-domain Question Answering</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#dureader" class="sidebar-link">DuReader</a></li><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#quasar" class="sidebar-link">Quasar</a></li><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#searchqa" class="sidebar-link">SearchQA</a></li></ul></li><li><a href="/static/dataset/english/question_answering.html#knowledge-base-question-answering" class="sidebar-link">Knowledge Base Question Answering</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/static/dataset/english/question_answering.html#qald-9" class="sidebar-link">QALD-9</a></li></ul></li></ul></section></li></ul> </aside></div> <main class="page"> <div class="theme-default-content content__default"><h1 id="question-answering"><a href="#question-answering" class="header-anchor">#</a> Question answering</h1> <p>Question answering is the task of answering a question.</p> <h3 id="table-of-contents"><a href="#table-of-contents" class="header-anchor">#</a> Table of contents</h3> <ul><li><a href="#arc">ARC</a></li> <li><a href="#sharc">ShARC</a></li> <li><a href="#reading-comprehension">Reading comprehension</a> <ul><li><a href="#clicr">CliCR</a></li> <li><a href="#cnn--daily-mail">CNN / Daily Mail</a></li> <li><a href="#codah">CODAH</a></li> <li><a href="#coqa">CoQA</a></li> <li><a href="#hotpotqa">HotpotQA</a></li> <li><a href="#ms-marco">MS MARCO</a></li> <li><a href="#multirc">MultiRC</a></li> <li><a href="#newsqa">NewsQA</a></li> <li><a href="#qangaroo">QAngaroo</a></li> <li><a href="#quac">QuAC</a></li> <li><a href="#race">RACE</a></li> <li><a href="#squad">SQuAD</a></li> <li><a href="#story-cloze-test">Story Cloze Test</a></li> <li><a href="#swag">SWAG</a></li> <li><a href="#recipeqa">Recipe QA</a></li> <li><a href="#narrativeqa">NarrativeQA</a></li> <li><a href="#duorc">DuoRC</a></li> <li><a href="#drop">DROP</a></li> <li><a href="#cosmos-qa">Cosmos QA</a></li></ul></li> <li><a href="#open-domain-question-answering">Open-domain Question Answering</a> <ul><li><a href="#dureader">DuReader</a></li> <li><a href="#quasar">Quasar</a></li> <li><a href="#searchqa">SearchQA</a></li></ul></li> <li><a href="#knowledge-base-question-answering">Knowledge Base Question Answering</a></li></ul> <h3 id="arc"><a href="#arc" class="header-anchor">#</a> ARC</h3> <p>The <a href="http://ai2-website.s3.amazonaws.com/publications/AI2ReasoningChallenge2018.pdf" target="_blank" rel="noopener noreferrer">AI2 Reasoning Challenge (ARC)<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>
dataset is a question answering, which contains 7,787 genuine grade-school level, multiple-choice science questions.
The dataset is partitioned into a Challenge Set and an Easy Set. The Challenge Set contains only questions
answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm. Models are evaluated
based on accuracy.</p> <p>A public leaderboard is available on the <a href="http://data.allenai.org/arc/" target="_blank" rel="noopener noreferrer">ARC website<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <h3 id="sharc"><a href="#sharc" class="header-anchor">#</a> ShARC</h3> <p><a href="https://arxiv.org/abs/1809.01494" target="_blank" rel="noopener noreferrer">ShARC<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> is a challenging QA dataset that requires  logical reasoning, elements of entailment/NLI and natural language generation.</p> <p>Most work in machine reading focuses on question answering problems where the answer is directly expressed in the text to read. However, many real-world question answering problems require the reading of text not because it contains the literal answer, but because it contains a recipe to derive an answer together with the reader's background knowledge. We formalise this task and introduce the challenging ShARC dataset with 32k task instances.</p> <p>The goal is to answer questions by possibly asking follow-up questions first. We assume that the question does not provide enough information to be answered directly. However, a model can use the supporting rule text to infer what needs to be asked in order to determine the final answer. Concretely, The model must decide whether to answer with &quot;Yes&quot;, &quot;No&quot;, &quot;Irrelevant&quot;, or to generate a follow-up question given rule text, a user scenario and a conversation history. Performance is measured with Micro and Macro Accuracy for &quot;Yes&quot;/&quot;No&quot;/&quot;Irrelevant&quot;/&quot;More&quot; classifications, and the quality of follow-up questions are measured with BLEU.</p> <p>The public data, further task details and public leaderboard are available on the <a href="https://sharc-data.github.io/" target="_blank" rel="noopener noreferrer">ShARC Website<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <h2 id="reading-comprehension"><a href="#reading-comprehension" class="header-anchor">#</a> Reading comprehension</h2> <p>Most current question answering datasets frame the task as reading comprehension where the question is about a paragraph
or document and the answer often is a span in the document. The Machine Reading group
at UCL also provides an <a href="https://uclnlp.github.io/ai4exams/data.html" target="_blank" rel="noopener noreferrer">overview of reading comprehension tasks<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <h3 id="clicr"><a href="#clicr" class="header-anchor">#</a> CliCR</h3> <p>The <a href="http://aclweb.org/anthology/N18-1140" target="_blank" rel="noopener noreferrer">CliCR dataset<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> is a gap-filling reading comprehension dataset consisting of around 100,000 queries and their associated documents. The dataset was built from clinical case reports, requiring the reader to answer the query with a medical problem/test/treatment entity. The abilities to perform bridging inferences and track objects have been found to be the most frequently required skills for successful answering.</p> <p>The instructions for accessing the dataset, the processing scripts, the baselines and the adaptations of some neural models can be found <a href="https://github.com/clips/clicr" target="_blank" rel="noopener noreferrer">here<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p>Example:</p> <table><thead><tr><th>Document</th> <th style="text-align:right;">Question</th> <th style="text-align:right;">Answer</th></tr></thead> <tbody><tr><td>We report a case of a 72-year-old Caucasian woman with pl-7 positive antisynthetase syndrome. Clinical presentation included interstitial lung disease, myositis, mechanic’s hands and dysphagia. As lung injury was the main concern, treatment consisted of prednisolone and cyclophosphamide. Complete remission with reversal of pulmonary damage was achieved, as reported by CT scan, pulmonary function tests and functional status. [...]</td> <td style="text-align:right;">Therefore, in severe cases an aggressive treatment, combining ________ and glucocorticoids as used in systemic vasculitis, is suggested.</td> <td style="text-align:right;">cyclophoshamide</td></tr></tbody></table> <table><thead><tr><th>Model</th> <th style="text-align:center;">F1</th> <th>Paper</th></tr></thead> <tbody><tr><td>Gated-Attention Reader (Dhingra et al., 2017)</td> <td style="text-align:center;">33.9</td> <td><a href="http://aclweb.org/anthology/N18-1140" target="_blank" rel="noopener noreferrer">CliCR: A Dataset of Clinical Case Reports for Machine Reading Comprehension<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>Stanford Attentive Reader (Chen et al., 2016)</td> <td style="text-align:center;">27.2</td> <td><a href="http://aclweb.org/anthology/N18-1140" target="_blank" rel="noopener noreferrer">CliCR: A Dataset of Clinical Case Reports for Machine Reading Comprehension<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr></tbody></table> <h3 id="cnn-daily-mail"><a href="#cnn-daily-mail" class="header-anchor">#</a> CNN / Daily Mail</h3> <p>The <a href="https://arxiv.org/abs/1506.03340" target="_blank" rel="noopener noreferrer">CNN / Daily Mail dataset<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> is a Cloze-style reading comprehension dataset
created from CNN and Daily Mail news articles using heuristics. <a href="https://en.wikipedia.org/wiki/Cloze_test" target="_blank" rel="noopener noreferrer">Close-style<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>
means that a missing word has to be inferred. In this case, &quot;questions&quot; were created by replacing entities
from bullet points summarizing one or several aspects of the article. Coreferent entities have been replaced with an
entity marker @entityn where n is a distinct index.
The model is tasked to infer the missing entity
in the bullet point based on the content of the corresponding article and models are evaluated based on
their accuracy on the test set.</p> <table><thead><tr><th></th> <th style="text-align:right;">CNN</th> <th style="text-align:right;">Daily Mail</th></tr></thead> <tbody><tr><td># Train</td> <td style="text-align:right;">380,298</td> <td style="text-align:right;">879,450</td></tr> <tr><td># Dev</td> <td style="text-align:right;">3,924</td> <td style="text-align:right;">64,835</td></tr> <tr><td># Test</td> <td style="text-align:right;">3,198</td> <td style="text-align:right;">53,182</td></tr></tbody></table> <p>Example:</p> <table><thead><tr><th>Passage</th> <th style="text-align:right;">Question</th> <th style="text-align:right;">Answer</th></tr></thead> <tbody><tr><td>( @entity4 ) if you feel a ripple in the force today , it may be the news that the official @entity6 is getting its first gay character . according to the sci-fi website @entity9 , the upcoming novel &quot; @entity11 &quot; will feature a capable but flawed @entity13 official named @entity14 who &quot; also happens to be a lesbian . &quot; the character is the first gay figure in the official @entity6 -- the movies , television shows , comics and books approved by @entity6 franchise owner @entity22 -- according to @entity24 , editor of &quot; @entity6 &quot; books at @entity28 imprint @entity26 .</td> <td style="text-align:right;">characters in &quot; @placeholder &quot; movies have gradually become more diverse</td> <td style="text-align:right;">@entity6</td></tr></tbody></table> <table><thead><tr><th>Model</th> <th style="text-align:center;">CNN</th> <th style="text-align:center;">Daily Mail</th> <th>Paper / Source</th></tr></thead> <tbody><tr><td>GA Reader(Dhingra et al., 2017)</td> <td style="text-align:center;">77.9</td> <td style="text-align:center;">80.9</td> <td><a href="http://aclweb.org/anthology/P17-1168" target="_blank" rel="noopener noreferrer">Gated-Attention Readers for Text Comprehension<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>BIDAF(Seo et al., 2017)</td> <td style="text-align:center;">76.9</td> <td style="text-align:center;">79.6</td> <td><a href="https://arxiv.org/pdf/1611.01603.pdf" target="_blank" rel="noopener noreferrer">Bidirectional Attention Flow for Machine Comprehension<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>AoA Reader(Cui et al., 2017)</td> <td style="text-align:center;">74.4</td> <td style="text-align:center;">-</td> <td><a href="http://aclweb.org/anthology/P17-1055" target="_blank" rel="noopener noreferrer">Attention-over-Attention Neural Networks for Reading Comprehension<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>Neural net (Chen et al., 2016)</td> <td style="text-align:center;">72.4</td> <td style="text-align:center;">75.8</td> <td><a href="https://www.aclweb.org/anthology/P16-1223" target="_blank" rel="noopener noreferrer">A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>Classifier (Chen et al., 2016)</td> <td style="text-align:center;">67.9</td> <td style="text-align:center;">68.3</td> <td><a href="https://www.aclweb.org/anthology/P16-1223" target="_blank" rel="noopener noreferrer">A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>Impatient Reader (Hermann et al., 2015)</td> <td style="text-align:center;">63.8</td> <td style="text-align:center;">68.0</td> <td><a href="https://arxiv.org/abs/1506.03340" target="_blank" rel="noopener noreferrer">Teaching Machines to Read and Comprehend<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr></tbody></table> <h3 id="codah"><a href="#codah" class="header-anchor">#</a> CODAH</h3> <p><a href="https://arxiv.org/abs/1904.04365" target="_blank" rel="noopener noreferrer">CODAH<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> is an adversarially-constructed evaluation dataset with 2.8k questions for testing common sense. CODAH forms a challenging extension to the SWAG dataset, which tests commonsense knowledge using sentence-completion questions that describe situations observed in video.</p> <p>The dataset and more information can be found <a href="https://github.com/Websail-NU/CODAH" target="_blank" rel="noopener noreferrer">here<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <h3 id="coqa"><a href="#coqa" class="header-anchor">#</a> CoQA</h3> <p><a href="https://arxiv.org/abs/1808.07042" target="_blank" rel="noopener noreferrer">CoQA<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> is a large-scale dataset for building Conversational Question Answering systems.
CoQA contains 127,000+ questions with answers collected from 8000+ conversations.
Each conversation is collected by pairing two crowdworkers to chat about a passage in the form of questions and answers.</p> <p>The data and public leaderboard are available <a href="https://stanfordnlp.github.io/coqa/" target="_blank" rel="noopener noreferrer">here<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <h3 id="hotpotqa"><a href="#hotpotqa" class="header-anchor">#</a> HotpotQA</h3> <p>HotpotQA is a dataset with 113k Wikipedia-based question-answer pairs. Questions require
finding and reasoning over multiple supporting documents and are not constrained to any pre-existing knowledge bases.
Sentence-level supporting facts are available.</p> <p>The data and public leaderboard are available from the <a href="https://hotpotqa.github.io/" target="_blank" rel="noopener noreferrer">HotpotQA website<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <h3 id="ms-marco"><a href="#ms-marco" class="header-anchor">#</a> MS MARCO</h3> <p><a href="http://www.msmarco.org/dataset.aspx" target="_blank" rel="noopener noreferrer">MS MARCO<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> aka Human Generated MAchine
Reading COmprehension Dataset, is designed and developed by Microsoft AI &amp; Research. <a href="https://arxiv.org/abs/1611.09268" target="_blank" rel="noopener noreferrer">Link to paper<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <ul><li>The questions are obtained from real anonymized user queries.</li> <li>The answers are human generated. The context passages from which the answers are obtained are extracted from real documents using the latest Bing search engine.</li> <li>The data set contains 100,000 queries and a subset of them contain multiple answers, and aim to release 1M queries in the future.</li></ul> <p>The leaderboards for multiple tasks are available on the <a href="http://www.msmarco.org/leaders.aspx" target="_blank" rel="noopener noreferrer">MS MARCO leaderboard page<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <h3 id="multirc"><a href="#multirc" class="header-anchor">#</a> MultiRC</h3> <p>MultiRC (Multi-Sentence Reading Comprehension) is a dataset of short paragraphs and multi-sentence questions that can be answered from the content of the paragraph.
We have designed the dataset with three key challenges in mind:</p> <ul><li>The number of correct answer-options for each question is not pre-specified. This removes the over-reliance of current approaches on answer-options and forces them to decide on the correctness of each candidate answer independently of others. In other words, unlike previous work, the task here is not to simply identify the best answer-option, but to evaluate the correctness of each answer-option individually.</li> <li>The correct answer(s) is not required to be a span in the text.</li> <li>The paragraphs in our dataset have diverse provenance by being extracted from 7 different domains such as news, fiction, historical text etc., and hence are expected to be more diverse in their contents as compared to single-domain datasets.</li></ul> <p>The leaderboards for the dataset is available on the <a href="http://cogcomp.org/multirc/" target="_blank" rel="noopener noreferrer">MultiRC website<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <h3 id="newsqa"><a href="#newsqa" class="header-anchor">#</a> NewsQA</h3> <p>The <a href="https://arxiv.org/pdf/1611.09830.pdf" target="_blank" rel="noopener noreferrer">NewsQA dataset<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> is a reading comprehension dataset of over 100,000
human-generated question-answer pairs from over 10,000 news articles from CNN, with answers consisting of spans of text
from the corresponding articles.
Some challenging characteristics of this dataset are:</p> <ul><li>Answers are spans of arbitrary length;</li> <li>Some questions have no answer in the corresponding article;</li> <li>There are no candidate answers from which to choose.
Although very similar to the SQuAD dataset, NewsQA offers a greater challenge to existing models at time of
introduction (eg. the paragraphs are longer than those in SQuAD). Models are evaluated based on F1 and Exact Match.</li></ul> <p>Example:</p> <table><thead><tr><th>Story</th> <th style="text-align:right;">Question</th> <th style="text-align:right;">Answer</th></tr></thead> <tbody><tr><td>MOSCOW, Russia (CNN) -- Russian space officials say the crew of the Soyuz space ship is resting after a rough ride back to Earth. A South Korean bioengineer was one of three people on board the Soyuz capsule. The craft carrying South Korea's first astronaut landed in northern Kazakhstan on Saturday, 260 miles (418 kilometers) off its mark, they said. Mission Control spokesman Valery Lyndin said the condition of the crew -- South Korean bioengineer Yi So-yeon, American astronaut Peggy Whitson and Russian flight engineer Yuri Malenchenko -- was satisfactory, though the three had been subjected to severe G-forces during the re-entry. [...]</td> <td style="text-align:right;">Where did the Soyuz capsule land?</td> <td style="text-align:right;">northern Kazakhstan</td></tr></tbody></table> <p>The dataset can be downloaded <a href="https://github.com/Maluuba/newsqa" target="_blank" rel="noopener noreferrer">here<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <table><thead><tr><th>Model</th> <th style="text-align:center;">F1</th> <th style="text-align:center;">EM</th> <th>Paper / Source</th></tr></thead> <tbody><tr><td>DecaProp (Tay et al., 2018)</td> <td style="text-align:center;">66.3</td> <td style="text-align:center;">53.1</td> <td><a href="https://arxiv.org/abs/1811.04210" target="_blank" rel="noopener noreferrer">Densely Connected Attention Propagation for Reading Comprehension<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>AMANDA (Kundu et al., 2018)</td> <td style="text-align:center;">63.7</td> <td style="text-align:center;">48.4</td> <td><a href="https://arxiv.org/abs/1801.08290" target="_blank" rel="noopener noreferrer">A Question-Focused Multi-Factor Attention Network for Question Answering<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>MINIMAL(Dyn) (Min et al., 2018)</td> <td style="text-align:center;">63.2</td> <td style="text-align:center;">50.1</td> <td><a href="https://arxiv.org/abs/1805.08092" target="_blank" rel="noopener noreferrer">Efficient and Robust Question Answering from Minimal Context over Documents<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>FastQAExt (Weissenborn et al., 2017)</td> <td style="text-align:center;">56.1</td> <td style="text-align:center;">43.7</td> <td><a href="https://arxiv.org/abs/1703.04816" target="_blank" rel="noopener noreferrer">Making Neural QA as Simple as Possible but not Simpler<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr></tbody></table> <h3 id="qangaroo"><a href="#qangaroo" class="header-anchor">#</a> QAngaroo</h3> <p><a href="http://qangaroo.cs.ucl.ac.uk/index.html" target="_blank" rel="noopener noreferrer">QAngaroo<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> is a set of two reading comprehension datasets,
which require multiple steps of inference that combine facts from multiple documents. The first dataset, WikiHop
is open-domain and focuses on Wikipedia articles. The second dataset, MedHop is based on paper abstracts from
PubMed.</p> <p>The leaderboards for both datasets are available on the <a href="http://qangaroo.cs.ucl.ac.uk/leaderboard.html" target="_blank" rel="noopener noreferrer">QAngaroo website<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <h3 id="quac"><a href="#quac" class="header-anchor">#</a> QuAC</h3> <p>Question Answering in Context (QuAC) is a dataset for modeling, understanding, and participating in information seeking dialog.
Data instances consist of an interactive dialog between two crowd workers:
(1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text,
and (2) a teacher who answers the questions by providing short excerpts (spans) from the text.</p> <p>The leaderboard and data are available on the <a href="http://quac.ai/" target="_blank" rel="noopener noreferrer">QuAC website<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <h3 id="race"><a href="#race" class="header-anchor">#</a> RACE</h3> <p>The <a href="https://arxiv.org/abs/1704.04683" target="_blank" rel="noopener noreferrer">RACE dataset<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> is a reading comprehension dataset
collected from English examinations in China, which are designed for middle school and high school students.
The dataset contains more than 28,000 passages and nearly 100,000 questions and can be
downloaded <a href="http://www.cs.cmu.edu/~glai1/data/race/" target="_blank" rel="noopener noreferrer">here<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>. Models are evaluated based on accuracy
on middle school examinations (RACE-m), high school examinations (RACE-h), and on the total dataset (RACE).</p> <p>The public leaderboard is available on the <a href="http://www.qizhexie.com//data/RACE_leaderboard" target="_blank" rel="noopener noreferrer">RACE leaderboard<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <table><thead><tr><th>Model</th> <th style="text-align:center;">RACE-m</th> <th style="text-align:center;">RACE-h</th> <th style="text-align:center;">RACE</th> <th>Paper</th> <th>Code</th></tr></thead> <tbody><tr><td>XLNet (Yang et al., 2019)</td> <td style="text-align:center;">85.45</td> <td style="text-align:center;">80.21</td> <td style="text-align:center;">81.75</td> <td><a href="https://arxiv.org/pdf/1906.08237.pdf" target="_blank" rel="noopener noreferrer">XLNet: Generalized Autoregressive Pretraining for Language Understanding<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td><a href="https://github.com/zihangdai/xlnet/" target="_blank" rel="noopener noreferrer">Official<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>OCN_large (Ran et al., 2019)</td> <td style="text-align:center;">76.7</td> <td style="text-align:center;">69.6</td> <td style="text-align:center;">71.7</td> <td><a href="https://arxiv.org/pdf/1903.03033.pdf" target="_blank" rel="noopener noreferrer">Option Comparison Network for Multiple-choice Reading Comprehension<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr> <tr><td>DCMN_large (Zhang et al., 2019)</td> <td style="text-align:center;">73.4</td> <td style="text-align:center;">68.1</td> <td style="text-align:center;">69.7</td> <td><a href="https://arxiv.org/pdf/1901.09381.pdf" target="_blank" rel="noopener noreferrer">Dual Co-Matching Network for Multi-choice Reading Comprehension<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr> <tr><td>Finetuned Transformer LM (Radford et al., 2018)</td> <td style="text-align:center;">62.9</td> <td style="text-align:center;">57.4</td> <td style="text-align:center;">59.0</td> <td><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener noreferrer">Improving Language Understanding by Generative Pre-Training<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td><a href="https://github.com/openai/finetune-transformer-lm" target="_blank" rel="noopener noreferrer">Official<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>BiAttention MRU (Tay et al., 2018)</td> <td style="text-align:center;">60.2</td> <td style="text-align:center;">50.3</td> <td style="text-align:center;">53.3</td> <td><a href="https://arxiv.org/abs/1803.09074" target="_blank" rel="noopener noreferrer">Multi-range Reasoning for Machine Comprehension<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr></tbody></table> <h3 id="squad"><a href="#squad" class="header-anchor">#</a> SQuAD</h3> <p>The <a href="https://arxiv.org/abs/1606.05250" target="_blank" rel="noopener noreferrer">Stanford Question Answering Dataset (SQuAD)<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>
is a reading comprehension dataset, consisting of questions posed by crowdworkers
on a set of Wikipedia articles. The answer to every question is a segment of text (a span)
from the corresponding reading passage. Recently, <a href="https://arxiv.org/abs/1806.03822" target="_blank" rel="noopener noreferrer">SQuAD 2.0<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>
has been released, which includes unanswerable questions.</p> <p>The public leaderboard is available on the <a href="https://rajpurkar.github.io/SQuAD-explorer/" target="_blank" rel="noopener noreferrer">SQuAD website<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <h3 id="story-cloze-test"><a href="#story-cloze-test" class="header-anchor">#</a> Story Cloze Test</h3> <p>The <a href="http://aclweb.org/anthology/W17-0906.pdf" target="_blank" rel="noopener noreferrer">Story Cloze Test<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> is a dataset for
story understanding that provides systems with four-sentence stories and two possible
endings. The systems must then choose the correct ending to the story.</p> <p>More details are available on the <a href="https://competitions.codalab.org/competitions/15333" target="_blank" rel="noopener noreferrer">Story Cloze Test Challenge<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <table><thead><tr><th>Model</th> <th style="text-align:center;">Accuracy</th> <th>Paper / Source</th> <th>Code</th></tr></thead> <tbody><tr><td>Reading Strategies Model (Sun et al., 2018)</td> <td style="text-align:center;">88.3</td> <td><a href="https://arxiv.org/pdf/1810.13441v1.pdf" target="_blank" rel="noopener noreferrer">Improving Machine Reading Comprehension by General Reading Strategies<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr> <tr><td>Finetuned Transformer LM (Radford et al., 2018)</td> <td style="text-align:center;">86.5</td> <td><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener noreferrer">Improving Language Understanding by Generative Pre-Training<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td><a href="https://github.com/openai/finetune-transformer-lm" target="_blank" rel="noopener noreferrer">Official<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>Liu et al. (2018)</td> <td style="text-align:center;">78.7</td> <td><a href="http://aclweb.org/anthology/P18-2045" target="_blank" rel="noopener noreferrer">Narrative Modeling with Memory Chains and Semantic Supervision<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td><a href="https://github.com/liufly/narrative-modeling" target="_blank" rel="noopener noreferrer">Official<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>Hidden Coherence Model (Chaturvedi et al., 2017)</td> <td style="text-align:center;">77.6</td> <td><a href="http://aclweb.org/anthology/D17-1168" target="_blank" rel="noopener noreferrer">Story Comprehension for Predicting What Happens Next<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr> <tr><td>val-LS-skip (Srinivasan et al., 2018)</td> <td style="text-align:center;">76.5</td> <td><a href="http://aclweb.org/anthology/N18-2015" target="_blank" rel="noopener noreferrer">A Simple and Effective Approach to the Story Cloze Test<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr></tbody></table> <h3 id="swag"><a href="#swag" class="header-anchor">#</a> SWAG</h3> <p><a href="https://arxiv.org/abs/1808.05326" target="_blank" rel="noopener noreferrer">SWAG<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> (Situations With Adversarial Generations) is a large-scale dataset for the task of grounded commonsense inference, unifying natural language inference and physically grounded reasoning. The dataset consists of 113k multiple choice questions about grounded situations. Each question is a video caption from LSMDC or ActivityNet Captions, with four answer choices about what might happen next in the scene. The correct answer is the (real) video caption for the next event in the video; the three incorrect answers are adversarially generated and human verified, so as to fool machines but not humans.</p> <p>The public leaderboard is available on the [AI2 website] (https://leaderboard.allenai.org/swag/submissions/public).</p> <h3 id="recipeqa"><a href="#recipeqa" class="header-anchor">#</a> RecipeQA</h3> <p><a href="https://arxiv.org/abs/1809.00812" target="_blank" rel="noopener noreferrer">RecipeQA<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> is a dataset for multimodal comprehension of cooking recipes. It consists of over 36K question-answer pairs automatically generated from approximately 20K unique recipes with step-by-step instructions and images. Each question in RecipeQA involves multiple modalities such as titles, descriptions or images, and working towards an answer requires (i) joint understanding of images and text, (ii) capturing the temporal flow of events, and (iii) making sense of procedural knowledge.</p> <p>The public leaderboard is available on the <a href="https://hucvl.github.io/recipeqa/" target="_blank" rel="noopener noreferrer">RecipeQA website<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <h3 id="narrativeqa"><a href="#narrativeqa" class="header-anchor">#</a> NarrativeQA</h3> <p><a href="https://arxiv.org/abs/1712.07040" target="_blank" rel="noopener noreferrer">NarrativeQA<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> is a dataset built to encourage deeper comprehension of language. This dataset involves reasoning over reading entire books or movie scripts. This dataset contains approximately 45K question answer pairs in free form text. There are two modes of this dataset (1) reading comprehension over summaries and (2) reading comprehension over entire books/scripts.</p> <table><thead><tr><th>Model</th> <th style="text-align:center;">BLEU-1</th> <th style="text-align:center;">BLEU-4</th> <th style="text-align:center;">METEOR</th> <th style="text-align:center;">Rouge-L</th> <th>Paper / Source</th> <th>Code</th></tr></thead> <tbody><tr><td>DecaProp (Tay et al., 2018)</td> <td style="text-align:center;">44.35</td> <td style="text-align:center;">27.61</td> <td style="text-align:center;">21.80</td> <td style="text-align:center;">44.69</td> <td><a href="https://arxiv.org/abs/1811.04210" target="_blank" rel="noopener noreferrer">Densely Connected Attention Propagation for Reading Comprehension<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td><a href="https://github.com/vanzytay/NIPS2018_DECAPROP" target="_blank" rel="noopener noreferrer">official<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>BiAttention + DCU-LSTM (Tay et al., 2018)</td> <td style="text-align:center;">36.55</td> <td style="text-align:center;">19.79</td> <td style="text-align:center;">17.87</td> <td style="text-align:center;">41.44</td> <td><a href="http://aclweb.org/anthology/D18-1238" target="_blank" rel="noopener noreferrer">Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr> <tr><td>BiDAF (Seo et al., 2017)</td> <td style="text-align:center;">33.45</td> <td style="text-align:center;">15.69</td> <td style="text-align:center;">15.68</td> <td style="text-align:center;">36.74</td> <td><a href="https://arxiv.org/abs/1611.01603" target="_blank" rel="noopener noreferrer">Bidirectional Attention Flow for Machine Comprehension<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr></tbody></table> <p>*Note that the above is for the Summary setting. There are no official published results for reading over entire books/stories except for the original paper.</p> <h3 id="duorc"><a href="#duorc" class="header-anchor">#</a> DuoRC</h3> <p><a href="https://duorc.github.io" target="_blank" rel="noopener noreferrer">DuoRC<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.</p> <p>DuoRC pushes the NLP community to address challenges on incorporating knowledge and reasoning in neural architectures for reading comprehension. It poses several interesting challenges such as:</p> <ul><li>DuoRC using parallel plots is especially designed to contain a large number of questions with low lexical overlap between questions and their corresponding passages</li> <li>It requires models to go beyond the content of the given passage itself and incorporate world-knowledge, background knowledge, and common-sense knowledge to arrive at the answer</li> <li>It revolves around narrative passages from movie plots describing complex events and therefore naturally require complex reasoning (e.g. temporal reasoning, entailment, long-distance anaphoras, etc.) across multiple sentences to infer the answer to questions</li> <li>Several of the questions in DuoRC, while seeming relevant, cannot actually be answered from the given passage. This requires the model to detect the unanswerability of questions. This aspect is important for machines to achieve in industrial settings in particular.</li></ul> <h3 id="drop"><a href="#drop" class="header-anchor">#</a> DROP</h3> <p><a href="https://allennlp.org/drop" target="_blank" rel="noopener noreferrer">DROP<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> is a crowdsourced, adversarially-created, 96k-question benchmark, in which a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets.</p> <h3 id="cosmos-qa"><a href="#cosmos-qa" class="header-anchor">#</a> Cosmos QA</h3> <p><a href="https://wilburone.github.io/cosmos/" target="_blank" rel="noopener noreferrer">Cosmos QA<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> is a large-scale dataset of 35.6K problems that require commonsense-based reading comprehension, formulated as multiple-choice questions. It focuses on reading between the lines over a diverse collection of people's everyday narratives, asking questions concerning on the likely causes or effects of events that require reasoning beyond the exact text spans in the context.</p> <h2 id="open-domain-question-answering"><a href="#open-domain-question-answering" class="header-anchor">#</a> Open-domain Question Answering</h2> <h3 id="dureader"><a href="#dureader" class="header-anchor">#</a> DuReader</h3> <p><a href="https://ai.baidu.com/broad/subordinate?dataset=dureader" target="_blank" rel="noopener noreferrer">DuReader<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> is a large-scale, open-domain Chinese machine reading comprehension (MRC) dataset, designed to address real-world MRC. <a href="https://arxiv.org/pdf/1711.05073.pdf" target="_blank" rel="noopener noreferrer">Link to paper<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>DuReader has three advantages over other MRC datasets:</p> <ul><li>(1) data sources: questions and documents are based on Baidu Search and Baidu Zhidao; answers are manually generated.</li> <li>(2) question types: it provides rich annotations for more question types, especially yes-no and opinion questions, that leaves more opportunity for the research community.</li> <li>(3) scale: it contains 300K questions, 660K answers and 1.5M documents; it is the largest Chinese MRC dataset so far.</li></ul> <p>To help the community make these improvements, both the <a href="https://ai.baidu.com/broad/download?dataset=dureader" target="_blank" rel="noopener noreferrer">dataset<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> of DuReader and <a href="https://github.com/baidu/DuReader" target="_blank" rel="noopener noreferrer">baseline systems<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> have been posted online.</p> <p>The <a href="https://ai.baidu.com/broad/leaderboard?dataset=dureader" target="_blank" rel="noopener noreferrer">leaderboard<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> is avaiable on DuReader page.</p> <h3 id="quasar"><a href="#quasar" class="header-anchor">#</a> Quasar</h3> <p><a href="https://arxiv.org/abs/1707.03904" target="_blank" rel="noopener noreferrer">Quasar<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> is a dataset for open-domain question answering. It includes two parts: (1) The Quasar-S dataset consists of 37,000 cloze-style queries constructed from definitions of software entity tags on the popular website Stack Overflow. (2) The Quasar-T dataset consists of 43,000 open-domain trivia questions and their answers obtained from various internet sources.</p> <table><thead><tr><th>Model</th> <th style="text-align:center;">EM (Quasar-T)</th> <th style="text-align:center;">F1 (Quasar-T)</th> <th>Paper / Source</th> <th>Code</th></tr></thead> <tbody><tr><td>Denoising QA (Lin et al. 2018)</td> <td style="text-align:center;">42.2</td> <td style="text-align:center;">49.3</td> <td><a href="http://aclweb.org/anthology/P18-1161" target="_blank" rel="noopener noreferrer">Denoising Distantly Supervised Open-Domain Question Answering<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td><a href="https://github.com/thunlp/OpenQA" target="_blank" rel="noopener noreferrer">official<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>DecaProp (Tay et al., 2018)</td> <td style="text-align:center;">38.6</td> <td style="text-align:center;">46.9</td> <td><a href="https://arxiv.org/abs/1811.04210" target="_blank" rel="noopener noreferrer">Densely Connected Attention Propagation for Reading Comprehension<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td><a href="https://github.com/vanzytay/NIPS2018_DECAPROP" target="_blank" rel="noopener noreferrer">official<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>R^3 (Wang et al., 2018)</td> <td style="text-align:center;">35.3</td> <td style="text-align:center;">41.7</td> <td><a href="https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16712/16165" target="_blank" rel="noopener noreferrer">R^3: Reinforced Ranker-Reader for Open-Domain Question Answering<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td><a href="https://github.com/shuohangwang/mprc" target="_blank" rel="noopener noreferrer">official<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>BiDAF (Seo et al., 2017)</td> <td style="text-align:center;">25.9</td> <td style="text-align:center;">28.5</td> <td><a href="https://arxiv.org/abs/1611.01603" target="_blank" rel="noopener noreferrer">Bidirectional Attention Flow for Machine Comprehensio<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td><a href="https://github.com/allenai/bi-att-flow" target="_blank" rel="noopener noreferrer">official<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>GA (Dhingra et al., 2017)</td> <td style="text-align:center;">26.4</td> <td style="text-align:center;">26.4</td> <td><a href="https://arxiv.org/pdf/1606.01549" target="_blank" rel="noopener noreferrer">Gated-Attention Readers for Text Comprehension<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr></tbody></table> <h3 id="searchqa"><a href="#searchqa" class="header-anchor">#</a> SearchQA</h3> <p><a href="https://arxiv.org/abs/1704.05179" target="_blank" rel="noopener noreferrer">SearchQA<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> was constructed to reflect a full pipeline of general question-answering. SearchQA consists of more than 140k question-answer pairs with each pair having 49.6 snippets on average. Each question-answer-context tuple of the SearchQA comes with additional meta-data such as the snippet's URL.</p> <table><thead><tr><th>Model</th> <th style="text-align:center;">Unigram Acc</th> <th style="text-align:center;">N-gram F1</th> <th style="text-align:center;">EM</th> <th style="text-align:center;">F1</th> <th>Paper / Source</th> <th>Code</th></tr></thead> <tbody><tr><td>DecaProp (Tay et al., 2018)</td> <td style="text-align:center;">62.2</td> <td style="text-align:center;">70.8</td> <td style="text-align:center;">56.8</td> <td style="text-align:center;">63.6</td> <td><a href="https://arxiv.org/abs/1811.04210" target="_blank" rel="noopener noreferrer">Densely Connected Attention Propagation for Reading Comprehension<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td><a href="https://github.com/vanzytay/NIPS2018_DECAPROP" target="_blank" rel="noopener noreferrer">official<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>Denoising QA (Lin et al. 2018)</td> <td style="text-align:center;">-</td> <td style="text-align:center;">-</td> <td style="text-align:center;">58.8</td> <td style="text-align:center;">64.5</td> <td><a href="http://aclweb.org/anthology/P18-1161" target="_blank" rel="noopener noreferrer">Denoising Distantly Supervised Open-Domain Question Answering<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td><a href="https://github.com/thunlp/OpenQA" target="_blank" rel="noopener noreferrer">official<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>R^3 (Wang et al., 2018)</td> <td style="text-align:center;">-</td> <td style="text-align:center;">-</td> <td style="text-align:center;">49.0</td> <td style="text-align:center;">55.3</td> <td><a href="https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16712/16165" target="_blank" rel="noopener noreferrer">R^3: Reinforced Ranker-Reader for Open-Domain Question Answering<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td><a href="https://github.com/shuohangwang/mprc" target="_blank" rel="noopener noreferrer">official<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>Bi-Attention + DCU-LSTM (Tay et al., 2018)</td> <td style="text-align:center;">49.4</td> <td style="text-align:center;">59.5</td> <td style="text-align:center;">-</td> <td style="text-align:center;">-</td> <td><a href="http://aclweb.org/anthology/D18-1238" target="_blank" rel="noopener noreferrer">Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr> <tr><td>AMANDA (Kundu et al., 2018)</td> <td style="text-align:center;">46.8</td> <td style="text-align:center;">56.6</td> <td style="text-align:center;">-</td> <td style="text-align:center;">-</td> <td><a href="https://arxiv.org/abs/1801.08290" target="_blank" rel="noopener noreferrer">A Question-Focused Multi-Factor Attention Network for Question Answering<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td><a href="https://github.com/nusnlp/amanda" target="_blank" rel="noopener noreferrer">official<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>Focused Hierarchical RNN	(Ke et al., 2018)</td> <td style="text-align:center;">46.8</td> <td style="text-align:center;">53.4</td> <td style="text-align:center;">-</td> <td style="text-align:center;">-</td> <td><a href="http://proceedings.mlr.press/v80/ke18a/ke18a.pdf" target="_blank" rel="noopener noreferrer">Focused Hierarchical RNNs for Conditional Sequence Processing<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr> <tr><td>ASR (Kadlec et al, 2016)</td> <td style="text-align:center;">41.3</td> <td style="text-align:center;">22.8</td> <td style="text-align:center;">-</td> <td style="text-align:center;">-</td> <td><a href="https://arxiv.org/abs/1603.01547" target="_blank" rel="noopener noreferrer">Text Understanding with the Attention Sum Reader Network<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td></td></tr></tbody></table> <h2 id="knowledge-base-question-answering"><a href="#knowledge-base-question-answering" class="header-anchor">#</a> Knowledge Base Question Answering</h2> <p>Knowledge Base Question Answering is the task of answering natural language question based on a knowledge base/knowledge graph such as <a href="https://wiki.dbpedia.org/" target="_blank" rel="noopener noreferrer">DBpedia<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> or <a href="https://www.wikidata.org/" target="_blank" rel="noopener noreferrer">Wikidata<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <h3 id="qald-9"><a href="#qald-9" class="header-anchor">#</a> QALD-9</h3> <p><a href="http://ceur-ws.org/Vol-2241/paper-06.pdf" target="_blank" rel="noopener noreferrer">QALD-9<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> is a manually curated superset of the previous eight editions of the <a href="http://2018.nliwod.org/challenge" target="_blank" rel="noopener noreferrer">Question Answering over Linked Data (QALD) challenge<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> published in 2018. It is constructed by human experts to cover a wide range of natural language to SPARQL conversions based on DBpedia 2016-10 knowledge base. Each question-answer-pair has additional meta-data. QALD-9 is best evaluated using the <a href="http://gerbil-qa.aksw.org/gerbil/config" target="_blank" rel="noopener noreferrer">GERBIL QA platform<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> for repeatability of the evaluation numbers.</p> <table><thead><tr><th>Annotator</th> <th style="text-align:center;">Macro P</th> <th style="text-align:center;">Macro R</th> <th style="text-align:center;">Macro F1</th> <th style="text-align:center;">Error Count</th> <th style="text-align:center;">Average Time/Doc ms</th> <th style="text-align:center;">Macro F1 QALD</th> <th>Paper (including links to webservices/source code)</th></tr></thead> <tbody><tr><td>Elon (WS)</td> <td style="text-align:center;">0.049</td> <td style="text-align:center;">0.053</td> <td style="text-align:center;">0.050</td> <td style="text-align:center;">2</td> <td style="text-align:center;">219</td> <td style="text-align:center;">0.100</td> <td></td></tr> <tr><td>QASystem (WS)</td> <td style="text-align:center;">0.097</td> <td style="text-align:center;">0.116</td> <td style="text-align:center;">0.098</td> <td style="text-align:center;">0</td> <td style="text-align:center;">1014</td> <td style="text-align:center;">0.200</td> <td></td></tr> <tr><td>TeBaQA (WS)</td> <td style="text-align:center;">0.129</td> <td style="text-align:center;">0.134</td> <td style="text-align:center;">0.130</td> <td style="text-align:center;">0</td> <td style="text-align:center;">2668</td> <td style="text-align:center;">0.222</td> <td></td></tr> <tr><td>wdaqua-core1 (DBpedia)</td> <td style="text-align:center;">0.261</td> <td style="text-align:center;">0.267</td> <td style="text-align:center;">0.250</td> <td style="text-align:center;">0</td> <td style="text-align:center;">661</td> <td style="text-align:center;">0.289</td> <td>Diefenbach, Dennis, Kamal Singh, and Pierre Maret. &quot;Wdaqua-core1: a question answering service for rdf knowledge bases.&quot; Companion of the The Web Conference 2018 on The Web Conference 2018. International World Wide Web Conferences Steering Committee, 2018.</td></tr> <tr><td>gAnswer (WS)</td> <td style="text-align:center;">0.293</td> <td style="text-align:center;">0.327</td> <td style="text-align:center;">0.298</td> <td style="text-align:center;">1</td> <td style="text-align:center;">3076</td> <td style="text-align:center;">0.430</td> <td>Zou, Lei, et al. &quot;Natural language question answering over RDF: a graph data driven approach.&quot; Proceedings of the 2014 ACM SIGMOD international conference on Management of data. ACM, 2014.</td></tr></tbody></table> <p><a href="/static/dataset/" class="router-link-active">Go back to the README</a></p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div> <footer class="bk-dark" data-v-1cbc1c23><div class="footer-content" data-v-1cbc1c23><p data-v-1cbc1c23>NLP-progress maintained by <a href="https://github.com/sebastianruder" data-v-1cbc1c23>sebastianruder</a></p> <p data-v-1cbc1c23>Published with <a href="https://github.com/sebastianruder" data-v-1cbc1c23>GitHub Pages</a></p></div></footer></div><div class="global-ui"></div></div>
    <script src="/static/dataset/assets/js/app.18b431e4.js" defer></script><script src="/static/dataset/assets/js/2.c3866e2d.js" defer></script><script src="/static/dataset/assets/js/3.d90b44d3.js" defer></script><script src="/static/dataset/assets/js/30.9bfdd5d3.js" defer></script>
  </body>
</html>
