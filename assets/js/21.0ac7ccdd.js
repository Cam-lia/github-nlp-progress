(window.webpackJsonp=window.webpackJsonp||[]).push([[21],{260:function(t,e,r){"use strict";r.r(e);var a=r(1),n=Object(a.a)({},(function(){var t=this,e=t.$createElement,r=t._self._c||e;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h1",{attrs:{id:"language-modeling"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#language-modeling"}},[t._v("#")]),t._v(" Language modeling")]),t._v(" "),r("p",[t._v("Language modeling is the task of predicting the next word or character in a document.")]),t._v(" "),r("p",[t._v("* indicates models using dynamic evaluation; where, at test time, models may adapt to seen tokens in order to improve performance on following tokens. ("),r("a",{attrs:{href:"https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Mikolov et al., (2010)"),r("OutboundLink")],1),t._v(", "),r("a",{attrs:{href:"https://arxiv.org/pdf/1709.07432",target:"_blank",rel:"noopener noreferrer"}},[t._v("Krause et al., (2017)"),r("OutboundLink")],1),t._v(")")]),t._v(" "),r("h2",{attrs:{id:"word-level-models"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#word-level-models"}},[t._v("#")]),t._v(" Word Level Models")]),t._v(" "),r("h3",{attrs:{id:"penn-treebank"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#penn-treebank"}},[t._v("#")]),t._v(" Penn Treebank")]),t._v(" "),r("p",[t._v("A common evaluation dataset for language modeling ist the Penn Treebank,\nas pre-processed by "),r("a",{attrs:{href:"https://www.isca-speech.org/archive/archive_papers/interspeech_2011/i11_0605.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Mikolov et al., (2011)"),r("OutboundLink")],1),t._v(".\nThe dataset consists of 929k training words, 73k validation words, and\n82k test words. As part of the pre-processing, words were lower-cased, numbers\nwere replaced with N, newlines were replaced with "),r("code",[t._v("<eos>")]),t._v(",\nand all other punctuation was removed. The vocabulary is\nthe most frequent 10k words with the rest of the tokens replaced by an "),r("code",[t._v("<unk>")]),t._v(" token.\nModels are evaluated based on perplexity, which is the average\nper-word log-probability (lower is better).")]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("Model")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Validation perplexity")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Test perplexity")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Number of params")]),t._v(" "),r("th",[t._v("Paper / Source")]),t._v(" "),r("th",[t._v("Code")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("Mogrifier LSTM + dynamic eval (Melis et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("44.9")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("44.8")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("24M")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://arxiv.org/abs/1909.01792",target:"_blank",rel:"noopener noreferrer"}},[t._v("Mogrifier LSTM"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/deepmind/lamb",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AdvSoft + AWD-LSTM-MoS + dynamic eval (Wang et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("46.63")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("46.01")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("22M")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://proceedings.mlr.press/v97/wang19f/wang19f.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Improving Neural Language Modeling via Adversarial Training"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/ChengyueGongR/advsoft",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("FRAGE + AWD-LSTM-MoS + dynamic eval (Gong et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("47.38")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("46.54")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("22M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1809.06858",target:"_blank",rel:"noopener noreferrer"}},[t._v("FRAGE: Frequency-Agnostic Word Representation"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/ChengyueGongR/Frequency-Agnostic",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM-DOC x5 (Takase et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("48.63")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("47.17")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("185M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1808.10143",target:"_blank",rel:"noopener noreferrer"}},[t._v("Direct Output Connection for a High-Rank Language Model"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/nttcslab-nlp/doc_lm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM-MoS + dynamic eval (Yang et al., 2018)*")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("48.33")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("47.69")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("22M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1711.03953",target:"_blank",rel:"noopener noreferrer"}},[t._v("Breaking the Softmax Bottleneck: A High-Rank RNN Language Model"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/zihangdai/mos",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Mogrifier LSTM (Melis et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("51.4")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("50.1")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("24M")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://arxiv.org/abs/1909.01792",target:"_blank",rel:"noopener noreferrer"}},[t._v("Mogrifier LSTM"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/deepmind/lamb",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM + dynamic eval (Krause et al., 2017)*")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("51.6")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("51.1")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("24M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1709.07432",target:"_blank",rel:"noopener noreferrer"}},[t._v("Dynamic Evaluation of Neural Sequence Models"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/benkrause/dynamic-evaluation",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM-DOC + Partial Shuffle (Press, 2019) "),r("em",[r("strong",[t._v("preprint")])])]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("53.79")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("52.00")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("23M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1903.04167",target:"_blank",rel:"noopener noreferrer"}},[t._v("Partially Shuffling the Training Data to Improve Language Models"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/ofirpress/PartialShuffle",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM-DOC (Takase et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("54.12")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("52.38")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("23M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1808.10143",target:"_blank",rel:"noopener noreferrer"}},[t._v("Direct Output Connection for a High-Rank Language Model"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/nttcslab-nlp/doc_lm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM + continuous cache pointer (Merity et al., 2017)*")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("53.9")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("52.8")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("24M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1708.02182",target:"_blank",rel:"noopener noreferrer"}},[t._v("Regularizing and Optimizing LSTM Language Models"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/salesforce/awd-lstm-lm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Trellis Network (Bai et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("54.19")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("34M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://openreview.net/pdf?id=HyeVtoRqtQ",target:"_blank",rel:"noopener noreferrer"}},[t._v("Trellis Networks for Sequence Modeling"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/locuslab/trellisnet",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM-MoS + ATOI (Kocher et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("56.44")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("54.33")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("22M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1909.08700",target:"_blank",rel:"noopener noreferrer"}},[t._v("Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/nkcr/overlap-ml",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM-MoS + finetune (Yang et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("56.54")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("54.44")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("22M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1711.03953",target:"_blank",rel:"noopener noreferrer"}},[t._v("Breaking the Softmax Bottleneck: A High-Rank RNN Language Model"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/zihangdai/mos",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Transformer-XL (Dai et al., 2018) "),r("em",[r("strong",[t._v("under review")])])]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("56.72")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("54.52")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("24M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1901.02860.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/kimiyoung/transformer-xl",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM-MoS (Yang et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("58.08")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("55.97")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("22M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1711.03953",target:"_blank",rel:"noopener noreferrer"}},[t._v("Breaking the Softmax Bottleneck: A High-Rank RNN Language Model"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/zihangdai/mos",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM 3-layer with Fraternal dropout (Zołna et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("58.9")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("56.8")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("24M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1711.00066.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Fraternal dropout"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/kondiz/fraternal-dropout",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM (Merity et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("60.0")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("57.3")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("24M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1708.02182",target:"_blank",rel:"noopener noreferrer"}},[t._v("Regularizing and Optimizing LSTM Language Models"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/salesforce/awd-lstm-lm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])])])]),t._v(" "),r("h3",{attrs:{id:"wikitext-2"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#wikitext-2"}},[t._v("#")]),t._v(" WikiText-2")]),t._v(" "),r("p",[r("a",{attrs:{href:"https://arxiv.org/abs/1609.07843",target:"_blank",rel:"noopener noreferrer"}},[t._v("WikiText-2"),r("OutboundLink")],1),t._v(" has been proposed as a more realistic\nbenchmark for language modeling than the pre-processed Penn Treebank. WikiText-2\nconsists of around 2 million words extracted from Wikipedia articles.")]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("Model")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Validation perplexity")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Test perplexity")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Number of params")]),t._v(" "),r("th",[t._v("Paper / Source")]),t._v(" "),r("th",[t._v("Code")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("Mogrifier LSTM + dynamic eval (Melis et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("40.2")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("38.6")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("35M")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://arxiv.org/abs/1909.01792",target:"_blank",rel:"noopener noreferrer"}},[t._v("Mogrifier LSTM"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/deepmind/lamb",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AdvSoft + AWD-LSTM-MoS + dynamic eval (Wang et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("40.27")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("38.65")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("35M")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://proceedings.mlr.press/v97/wang19f/wang19f.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Improving Neural Language Modeling via Adversarial Training"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/ChengyueGongR/advsoft",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("FRAGE + AWD-LSTM-MoS + dynamic eval (Gong et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("40.85")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("39.14")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("35M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1809.06858",target:"_blank",rel:"noopener noreferrer"}},[t._v("FRAGE: Frequency-Agnostic Word Representation"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/ChengyueGongR/Frequency-Agnostic",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM-MoS + dynamic eval (Yang et al., 2018)*")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("42.41")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("40.68")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("35M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1711.03953",target:"_blank",rel:"noopener noreferrer"}},[t._v("Breaking the Softmax Bottleneck: A High-Rank RNN Language Model"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/zihangdai/mos",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM + dynamic eval (Krause et al., 2017)*")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("46.4")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("44.3")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("33M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1709.07432",target:"_blank",rel:"noopener noreferrer"}},[t._v("Dynamic Evaluation of Neural Sequence Models"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/benkrause/dynamic-evaluation",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM + continuous cache pointer (Merity et al., 2017)*")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("53.8")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("52.0")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("33M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1708.02182",target:"_blank",rel:"noopener noreferrer"}},[t._v("Regularizing and Optimizing LSTM Language Models"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/salesforce/awd-lstm-lm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM-DOC x5 (Takase et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("54.19")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("53.09")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("185M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1808.10143",target:"_blank",rel:"noopener noreferrer"}},[t._v("Direct Output Connection for a High-Rank Language Model"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/nttcslab-nlp/doc_lm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Mogrifier LSTM (Melis et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("57.3")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("55.1")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("35M")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://arxiv.org/abs/1909.01792",target:"_blank",rel:"noopener noreferrer"}},[t._v("Mogrifier LSTM"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/deepmind/lamb",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM-DOC + Partial Shuffle (Press, 2019) "),r("em",[r("strong",[t._v("preprint")])])]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("60.16")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("57.85")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1903.04167",target:"_blank",rel:"noopener noreferrer"}},[t._v("Partially Shuffling the Training Data to Improve Language Models"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/ofirpress/PartialShuffle",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM-DOC (Takase et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("60.29")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("58.03")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1808.10143",target:"_blank",rel:"noopener noreferrer"}},[t._v("Direct Output Connection for a High-Rank Language Model"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/nttcslab-nlp/doc_lm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM-MoS (Yang et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("63.88")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("61.45")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("35M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1711.03953",target:"_blank",rel:"noopener noreferrer"}},[t._v("Breaking the Softmax Bottleneck: A High-Rank RNN Language Model"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/zihangdai/mos",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM 3-layer with Fraternal dropout (Zołna et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("66.8")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("64.1")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("34M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1711.00066.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Fraternal dropout"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/kondiz/fraternal-dropout",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM + ATOI (Kocher et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("67.47")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("64.73")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("33M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1909.08700",target:"_blank",rel:"noopener noreferrer"}},[t._v("Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/nkcr/overlap-ml",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM (Merity et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("68.6")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("65.8")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("33M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1708.02182",target:"_blank",rel:"noopener noreferrer"}},[t._v("Regularizing and Optimizing LSTM Language Models"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/salesforce/awd-lstm-lm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])])])]),t._v(" "),r("h3",{attrs:{id:"wikitext-103"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#wikitext-103"}},[t._v("#")]),t._v(" WikiText-103")]),t._v(" "),r("p",[r("a",{attrs:{href:"https://arxiv.org/abs/1609.07843",target:"_blank",rel:"noopener noreferrer"}},[t._v("WikiText-103"),r("OutboundLink")],1),t._v(" The WikiText-103 corpus contains 267,735 unique words and each word occurs at least three times in the training set.")]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("Model")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Validation perplexity")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Test perplexity")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Number of params")]),t._v(" "),r("th",[t._v("Paper / Source")]),t._v(" "),r("th",[t._v("Code")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("Transformer-XL + RMS dynamic eval (Krause et al., 2019)* "),r("em",[r("strong",[t._v("arxiv preprint")])])]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("15.8")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("16.4")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("257M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1904.08378.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Dynamic Evaluation of Transformer Language Models"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/benkrause/dynamiceval-transformer",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Compressive Transformer (Rae et al., 2019)* "),r("em",[r("strong",[t._v("arxiv preprint")])])]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("16.0")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("17.1(16.1 with basic dynamic evaluation)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("~257M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1911.05507.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Compressive Transformers for Long-Range Sequence Modelling"),r("OutboundLink")],1)]),t._v(" "),r("td",[t._v("-")])]),t._v(" "),r("tr",[r("td",[t._v("Transformer-XL Large (Dai et al., 2018) "),r("em",[r("strong",[t._v("under review")])])]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("17.7")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("18.3")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("257M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1901.02860.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/kimiyoung/transformer-xl",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Transformer with tied adaptive embeddings (Baevski and Auli, 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("19.8")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("20.5")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("247M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1809.10853.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Adaptive Input Representations for Neural Language Modeling"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/AranKomat/adapinp",target:"_blank",rel:"noopener noreferrer"}},[t._v("Link"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Transformer-XL Standard (Dai et al., 2018) "),r("em",[r("strong",[t._v("under review")])])]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("23.1")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("24.0")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("151M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1901.02860.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/kimiyoung/transformer-xl",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AdvSoft + 4 layer QRNN + dynamic eval (Wang et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("27.2")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("28.0")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}}),t._v(" "),r("td",[r("a",{attrs:{href:"http://proceedings.mlr.press/v97/wang19f/wang19f.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Improving Neural Language Modeling via Adversarial Training"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/ChengyueGongR/advsoft",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("LSTM + Hebbian + Cache + MbPA (Rae et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("29.0")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("29.2")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}}),t._v(" "),r("td",[r("a",{attrs:{href:"http://arxiv.org/abs/1803.10049",target:"_blank",rel:"noopener noreferrer"}},[t._v("Fast Parametric Learning with Activation Memorization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("Trellis Network (Bai et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("30.35")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("180M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://openreview.net/pdf?id=HyeVtoRqtQ",target:"_blank",rel:"noopener noreferrer"}},[t._v("Trellis Networks for Sequence Modeling"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/locuslab/trellisnet",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AWD-LSTM-MoS + ATOI (Kocher et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("31.92")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("32.85")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}}),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1909.08700",target:"_blank",rel:"noopener noreferrer"}},[t._v("Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/nkcr/overlap-ml",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("LSTM + Hebbian (Rae et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("34.1")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("34.3")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}}),t._v(" "),r("td",[r("a",{attrs:{href:"http://arxiv.org/abs/1803.10049",target:"_blank",rel:"noopener noreferrer"}},[t._v("Fast Parametric Learning with Activation Memorization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("LSTM (Rae et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("36.0")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("36.4")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}}),t._v(" "),r("td",[r("a",{attrs:{href:"http://arxiv.org/abs/1803.10049",target:"_blank",rel:"noopener noreferrer"}},[t._v("Fast Parametric Learning with Activation Memorization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("Gated CNN (Dauphin et al., 2016)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37.2")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}}),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1612.08083",target:"_blank",rel:"noopener noreferrer"}},[t._v("Language modeling with gated convolutional networks"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("Neural cache model (size = 2,000) (Grave et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("40.8")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}}),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1612.04426.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Improving Neural Language Models with a Continuous Cache"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/kaishengtai/torch-ntm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Link"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Temporal CNN (Bai et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("45.2")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}}),t._v(" "),r("td",[r("a",{attrs:{href:"https://openreview.net/forum?id=BJEX-H1Pf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Convolutional sequence modeling revisited"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("LSTM (Grave et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("48.7")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}}),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1612.04426.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Improving Neural Language Models with a Continuous Cache"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/kaishengtai/torch-ntm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Link"),r("OutboundLink")],1)])])])]),t._v(" "),r("h3",{attrs:{id:"_1b-words-google-billion-word-benchmark"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1b-words-google-billion-word-benchmark"}},[t._v("#")]),t._v(" 1B Words / Google Billion Word benchmark")]),t._v(" "),r("p",[r("a",{attrs:{href:"https://arxiv.org/pdf/1312.3005.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("The One-Billion Word benchmark"),r("OutboundLink")],1),t._v(" is a large dataset derived from a news-commentary site.\nThe dataset consists of 829,250,940 tokens over a vocabulary of 793,471 words.\nImportantly, sentences in this model are shuffled and hence context is limited.")]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("Model")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Test perplexity")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Number of params")]),t._v(" "),r("th",[t._v("Paper / Source")]),t._v(" "),r("th",[t._v("Code")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("Transformer-XL Large (Dai et al., 2018) "),r("em",[r("strong",[t._v("under review")])])]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("21.8")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("0.8B")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1901.02860.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/kimiyoung/transformer-xl",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Transformer-XL Base (Dai et al., 2018) "),r("em",[r("strong",[t._v("under review")])])]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("23.5")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("0.46B")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1901.02860.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/kimiyoung/transformer-xl",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Transformer with shared adaptive embeddings - Very large (Baevski and Auli, 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("23.7")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("0.8B")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1809.10853.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Adaptive Input Representations for Neural Language Modeling"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/AranKomat/adapinp",target:"_blank",rel:"noopener noreferrer"}},[t._v("Link"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("10 LSTM+CNN inputs + SNM10-SKIP (Jozefowicz et al., 2016) "),r("em",[r("strong",[t._v("ensemble")])])]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("23.7")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("43B?")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1602.02410.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Exploring the Limits of Language Modeling"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/rafaljozefowicz/lm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Transformer with shared adaptive embeddings (Baevski and Auli, 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("24.1")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("0.46B")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1809.10853.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Adaptive Input Representations for Neural Language Modeling"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/AranKomat/adapinp",target:"_blank",rel:"noopener noreferrer"}},[t._v("Link"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Big LSTM+CNN inputs (Jozefowicz et al., 2016)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("30.0")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.04B")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1602.02410.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Exploring the Limits of Language Modeling"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("Gated CNN-14Bottleneck (Dauphin et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("31.9")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("?")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1612.08083.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Language Modeling with Gated Convolutional Networks"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("BIGLSTM baseline (Kuchaiev and Ginsburg, 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("35.1")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("0.151B")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1703.10722.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Factorization tricks for LSTM networks"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/okuchaiev/f-lm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("BIG F-LSTM F512 (Kuchaiev and Ginsburg, 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("36.3")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("0.052B")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1703.10722.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Factorization tricks for LSTM networks"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/okuchaiev/f-lm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("BIG G-LSTM G-8 (Kuchaiev and Ginsburg, 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("39.4")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("0.035B")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1703.10722.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Factorization tricks for LSTM networks"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/okuchaiev/f-lm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])])])]),t._v(" "),r("h2",{attrs:{id:"character-level-models"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#character-level-models"}},[t._v("#")]),t._v(" Character Level Models")]),t._v(" "),r("h3",{attrs:{id:"hutter-prize"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#hutter-prize"}},[t._v("#")]),t._v(" Hutter Prize")]),t._v(" "),r("p",[r("a",{attrs:{href:"http://prize.hutter1.net",target:"_blank",rel:"noopener noreferrer"}},[t._v("The Hutter Prize"),r("OutboundLink")],1),t._v(" Wikipedia dataset, also known as enwiki8, is a byte-level dataset consisting of the\nfirst 100 million bytes of a Wikipedia XML dump. For simplicity we shall refer to it as a character-level dataset.\nWithin these 100 million bytes are 205 unique tokens.")]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("Model")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Bit per Character (BPC)")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Number of params")]),t._v(" "),r("th",[t._v("Paper / Source")]),t._v(" "),r("th",[t._v("Code")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("Transformer-XL + RMS dynamic eval (Krause et al., 2019)* "),r("em",[r("strong",[t._v("arxiv preprint")])])]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("0.94")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("277M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1904.08378.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Dynamic Evaluation of Transformer Language Models"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/benkrause/dynamiceval-transformer",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Compressive Transformer (Rae et al., 2019) "),r("em",[r("strong",[t._v("arxiv preprint")])])]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("0.97")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1911.05507.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Compressive Transformers for Long-Range Sequence Modelling"),r("OutboundLink")],1)]),t._v(" "),r("td",[t._v("-")])]),t._v(" "),r("tr",[r("td",[t._v("Mogrifier LSTM + dynamic eval (Melis et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("0.988")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("96M")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://arxiv.org/abs/1909.01792",target:"_blank",rel:"noopener noreferrer"}},[t._v("Mogrifier LSTM"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/deepmind/lamb",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("24-layer Transformer-XL (Dai et al., 2018) "),r("em",[r("strong",[t._v("under review")])])]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("0.99")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("277M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1901.02860.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/kimiyoung/transformer-xl",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("18-layer Transformer-XL (Dai et al., 2018) "),r("em",[r("strong",[t._v("under review")])])]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.03")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("88M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1901.02860.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/kimiyoung/transformer-xl",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("12-layer Transformer-XL (Dai et al., 2018) "),r("em",[r("strong",[t._v("under review")])])]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.06")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("41M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1901.02860.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/kimiyoung/transformer-xl",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("64-layer Character Transformer Model (Al-Rfou et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.06")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("235M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1808.04444",target:"_blank",rel:"noopener noreferrer"}},[t._v("Character-Level Language Modeling with Deeper Self-Attention"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("mLSTM + dynamic eval (Krause et al., 2017)*")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.08")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("46M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1709.07432",target:"_blank",rel:"noopener noreferrer"}},[t._v("Dynamic Evaluation of Neural Sequence Models"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/benkrause/dynamic-evaluation",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("12-layer Character Transformer Model (Al-Rfou et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.11")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("44M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1808.04444",target:"_blank",rel:"noopener noreferrer"}},[t._v("Character-Level Language Modeling with Deeper Self-Attention"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("Mogrifier LSTM (Melis et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.122")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("96M")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://arxiv.org/abs/1909.01792",target:"_blank",rel:"noopener noreferrer"}},[t._v("Mogrifier LSTM"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/deepmind/lamb",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("3-layer AWD-LSTM (Merity et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.232")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("47M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1803.08240",target:"_blank",rel:"noopener noreferrer"}},[t._v("An Analysis of Neural Language Modeling at Multiple Scales"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/salesforce/awd-lstm-lm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Large mLSTM +emb +WN +VD (Krause et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.24")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("46M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1609.07959",target:"_blank",rel:"noopener noreferrer"}},[t._v("Multiplicative LSTM for sequence modelling"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/benkrause/mLSTM",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Large FS-LSTM-4 (Mujika et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.245")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("47M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1705.08639",target:"_blank",rel:"noopener noreferrer"}},[t._v("Fast-Slow Recurrent Neural Networks"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/amujika/Fast-Slow-LSTM",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Large RHN (Zilly et al., 2016)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.27")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("46M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1607.03474",target:"_blank",rel:"noopener noreferrer"}},[t._v("Recurrent Highway Networks"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/jzilly/RecurrentHighwayNetworks",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("FS-LSTM-4 (Mujika et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.277")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("27M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1705.08639",target:"_blank",rel:"noopener noreferrer"}},[t._v("Fast-Slow Recurrent Neural Networks"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/amujika/Fast-Slow-LSTM",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])])])]),t._v(" "),r("h3",{attrs:{id:"text8"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#text8"}},[t._v("#")]),t._v(" Text8")]),t._v(" "),r("p",[r("a",{attrs:{href:"http://mattmahoney.net/dc/textdata.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("The text8 dataset"),r("OutboundLink")],1),t._v(" is also derived from Wikipedia text, but has all XML removed, and is lower cased to only have 26 characters of English text plus spaces.")]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("Model")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Bit per Character (BPC)")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Number of params")]),t._v(" "),r("th",[t._v("Paper / Source")]),t._v(" "),r("th",[t._v("Code")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("Transformer-XL + RMS dynamic eval (Krause et al., 2019)* "),r("em",[r("strong",[t._v("arxiv preprint")])])]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.038")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("277M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1904.08378.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Dynamic Evaluation of Transformer Language Models"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/benkrause/dynamiceval-transformer",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Transformer-XL Large (Dai et al., 2018) "),r("em",[r("strong",[t._v("under review")])])]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.08")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("277M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1901.02860.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/kimiyoung/transformer-xl",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("64-layer Character Transformer Model (Al-Rfou et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.13")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("235M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1808.04444",target:"_blank",rel:"noopener noreferrer"}},[t._v("Character-Level Language Modeling with Deeper Self-Attention"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("12-layer Character Transformer Model (Al-Rfou et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.18")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("44M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1808.04444",target:"_blank",rel:"noopener noreferrer"}},[t._v("Character-Level Language Modeling with Deeper Self-Attention"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("mLSTM + dynamic eval (Krause et al., 2017)*")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.19")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("45M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1709.07432",target:"_blank",rel:"noopener noreferrer"}},[t._v("Dynamic Evaluation of Neural Sequence Models"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/benkrause/dynamic-evaluation",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Large mLSTM +emb +WN +VD (Krause et al., 2016)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.27")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("45M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1609.07959",target:"_blank",rel:"noopener noreferrer"}},[t._v("Multiplicative LSTM for sequence modelling"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/benkrause/mLSTM",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Large RHN (Zilly et al., 2016)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.27")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("46M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1607.03474",target:"_blank",rel:"noopener noreferrer"}},[t._v("Recurrent Highway Networks"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/jzilly/RecurrentHighwayNetworks",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("LayerNorm HM-LSTM (Chung et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.29")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("35M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1609.01704",target:"_blank",rel:"noopener noreferrer"}},[t._v("Hierarchical Multiscale Recurrent Neural Networks"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("BN LSTM (Cooijmans et al., 2016)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.36")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("16M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1603.09025",target:"_blank",rel:"noopener noreferrer"}},[t._v("Recurrent Batch Normalization"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/cooijmanstim/recurrent-batch-normalization",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Unregularised mLSTM (Krause et al., 2016)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.40")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("45M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1609.07959",target:"_blank",rel:"noopener noreferrer"}},[t._v("Multiplicative LSTM for sequence modelling"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/benkrause/mLSTM",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])])])]),t._v(" "),r("h3",{attrs:{id:"penn-treebank-2"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#penn-treebank-2"}},[t._v("#")]),t._v(" Penn Treebank")]),t._v(" "),r("p",[t._v("The vocabulary of the words in the character-level dataset is limited to 10 000 - the same vocabulary as used in the word level dataset.  This vastly simplifies the task of character-level language modeling as character transitions will be limited to those found within the limited word level vocabulary.")]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("Model")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Bit per Character (BPC)")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Number of params")]),t._v(" "),r("th",[t._v("Paper / Source")]),t._v(" "),r("th",[t._v("Code")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("Mogrifier LSTM + dynamic eval (Melis et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.083")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("24M")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://arxiv.org/abs/1909.01792",target:"_blank",rel:"noopener noreferrer"}},[t._v("Mogrifier LSTM"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/deepmind/lamb",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Mogrifier LSTM (Melis et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.120")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("24M")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://arxiv.org/abs/1909.01792",target:"_blank",rel:"noopener noreferrer"}},[t._v("Mogrifier LSTM"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/deepmind/lamb",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Trellis Network (Bai et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.159")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("13.4M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://openreview.net/pdf?id=HyeVtoRqtQ",target:"_blank",rel:"noopener noreferrer"}},[t._v("Trellis Networks for Sequence Modeling"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/locuslab/trellisnet",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("3-layer AWD-LSTM (Merity et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.175")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("13.8M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1803.08240",target:"_blank",rel:"noopener noreferrer"}},[t._v("An Analysis of Neural Language Modeling at Multiple Scales"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/salesforce/awd-lstm-lm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("6-layer QRNN (Merity et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.187")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("13.8M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1803.08240",target:"_blank",rel:"noopener noreferrer"}},[t._v("An Analysis of Neural Language Modeling at Multiple Scales"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/salesforce/awd-lstm-lm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("FS-LSTM-4 (Mujika et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.190")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("27M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1705.08639",target:"_blank",rel:"noopener noreferrer"}},[t._v("Fast-Slow Recurrent Neural Networks"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/amujika/Fast-Slow-LSTM",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("FS-LSTM-2 (Mujika et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.193")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("27M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1705.08639",target:"_blank",rel:"noopener noreferrer"}},[t._v("Fast-Slow Recurrent Neural Networks"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/amujika/Fast-Slow-LSTM",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("NASCell (Zoph & Le, 2016)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.214")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("16.3M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1611.01578",target:"_blank",rel:"noopener noreferrer"}},[t._v("Neural Architecture Search with Reinforcement Learning"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("2-layer Norm HyperLSTM (Ha et al., 2016)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.219")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("14.4M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1609.09106",target:"_blank",rel:"noopener noreferrer"}},[t._v("HyperNetworks"),r("OutboundLink")],1)]),t._v(" "),r("td")])])]),t._v(" "),r("h3",{attrs:{id:"multilingual-wikipedia-corpus"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#multilingual-wikipedia-corpus"}},[t._v("#")]),t._v(" Multilingual Wikipedia Corpus")]),t._v(" "),r("p",[t._v("The character-based "),r("a",{attrs:{href:"http://k-kawakami.com/research/mwc",target:"_blank",rel:"noopener noreferrer"}},[t._v("MWC"),r("OutboundLink")],1),t._v(" dataset is a collection of Wikipedia pages available in a number of languages. Markup and rare characters were removed, but otherwise no preprocessing was applied.")]),t._v(" "),r("h4",{attrs:{id:"mwc-english-in-the-single-text-large-setting"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#mwc-english-in-the-single-text-large-setting"}},[t._v("#")]),t._v(" MWC English in the single text, large setting.")]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("Model")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Validation BPC")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Test BPC")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Number of params")]),t._v(" "),r("th",[t._v("Paper / Source")]),t._v(" "),r("th",[t._v("Code")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("Mogrifier LSTM + dynamic eval (Melis et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.200")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.187")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("24M")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://arxiv.org/abs/1909.01792",target:"_blank",rel:"noopener noreferrer"}},[t._v("Mogrifier LSTM"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/deepmind/lamb",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Mogrifier LSTM (Melis et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.312")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.298")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("24M")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://arxiv.org/abs/1909.01792",target:"_blank",rel:"noopener noreferrer"}},[t._v("Mogrifier LSTM"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/deepmind/lamb",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("HCLM with Cache (Kawakami et al. 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.591")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.538")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("8M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1704.06986",target:"_blank",rel:"noopener noreferrer"}},[t._v("Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("LSTM (Kawakami et al. 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.793")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.736")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("8M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1704.06986",target:"_blank",rel:"noopener noreferrer"}},[t._v("Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling"),r("OutboundLink")],1)]),t._v(" "),r("td")])])]),t._v(" "),r("h4",{attrs:{id:"mwc-finnish-in-the-single-text-large-setting"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#mwc-finnish-in-the-single-text-large-setting"}},[t._v("#")]),t._v(" MWC Finnish in the single text, large setting.")]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("Model")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Validation BPC")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Test BPC")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Number of params")]),t._v(" "),r("th",[t._v("Paper / Source")]),t._v(" "),r("th",[t._v("Code")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("Mogrifier LSTM + dynamic eval (Melis et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.202")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.191")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("24M")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://arxiv.org/abs/1909.01792",target:"_blank",rel:"noopener noreferrer"}},[t._v("Mogrifier LSTM"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/deepmind/lamb",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Mogrifier LSTM (Melis et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.327")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.313")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("24M")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://arxiv.org/abs/1909.01792",target:"_blank",rel:"noopener noreferrer"}},[t._v("Mogrifier LSTM"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/deepmind/lamb",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("HCLM with Cache (Kawakami et al. 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.754")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.711")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("8M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1704.06986",target:"_blank",rel:"noopener noreferrer"}},[t._v("Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("LSTM (Kawakami et al. 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.943")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.913")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("8M")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1704.06986",target:"_blank",rel:"noopener noreferrer"}},[t._v("Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling"),r("OutboundLink")],1)]),t._v(" "),r("td")])])]),t._v(" "),r("p",[r("RouterLink",{attrs:{to:"/"}},[t._v("Go back to the README")])],1)])}),[],!1,null,null,null);e.default=n.exports}}]);