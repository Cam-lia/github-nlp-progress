(window.webpackJsonp=window.webpackJsonp||[]).push([[23],{246:function(t,e,r){"use strict";r.r(e);var a=r(1),n=Object(a.a)({},(function(){var t=this,e=t.$createElement,r=t._self._c||e;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h1",{attrs:{id:"machine-translation"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#machine-translation"}},[t._v("#")]),t._v(" Machine translation")]),t._v(" "),r("p",[t._v("Machine translation is the task of translating a sentence in a source language to a different target language.")]),t._v(" "),r("p",[t._v("Results with a * indicate that the mean test score over the the best window based on average dev-set BLEU score over\n21 consecutive evaluations is reported as in "),r("a",{attrs:{href:"https://arxiv.org/abs/1804.09849",target:"_blank",rel:"noopener noreferrer"}},[t._v("Chen et al. (2018)"),r("OutboundLink")],1),t._v(".")]),t._v(" "),r("h3",{attrs:{id:"wmt-2014-en-de"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#wmt-2014-en-de"}},[t._v("#")]),t._v(" WMT 2014 EN-DE")]),t._v(" "),r("p",[t._v("Models are evaluated on the English-German dataset of the Ninth Workshop on Statistical Machine Translation (WMT 2014) based\non BLEU.")]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("Model")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("BLEU")]),t._v(" "),r("th",[t._v("Paper / Source")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("Transformer Big + BT (Edunov et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("35.0")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1808.09381.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Understanding Back-Translation at Scale"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("DeepL")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("33.3")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://www.deepl.com/press.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("DeepL Press release"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("MUSE (Zhao et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("29.9")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1911.09483",target:"_blank",rel:"noopener noreferrer"}},[t._v("MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("DynamicConv (Wu et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("29.7")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1901.10430",target:"_blank",rel:"noopener noreferrer"}},[t._v("Pay Less Attention With Lightweight and Dynamic Convolutions"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("AdvSoft + Transformer Big (Wang et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("29.52")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://proceedings.mlr.press/v97/wang19f/wang19f.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Improving Neural Language Modeling via Adversarial Training"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Transformer Big (Ott et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("29.3")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1806.00187",target:"_blank",rel:"noopener noreferrer"}},[t._v("Scaling Neural Machine Translation"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("RNMT+ (Chen et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("28.5*")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1804.09849",target:"_blank",rel:"noopener noreferrer"}},[t._v("The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Transformer Big (Vaswani et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("28.4")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1706.03762",target:"_blank",rel:"noopener noreferrer"}},[t._v("Attention Is All You Need"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Transformer Base (Vaswani et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("27.3")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1706.03762",target:"_blank",rel:"noopener noreferrer"}},[t._v("Attention Is All You Need"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("MoE (Shazeer et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("26.03")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1701.06538",target:"_blank",rel:"noopener noreferrer"}},[t._v("Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("ConvS2S (Gehring et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("25.16")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1705.03122",target:"_blank",rel:"noopener noreferrer"}},[t._v("Convolutional Sequence to Sequence Learning"),r("OutboundLink")],1)])])])]),t._v(" "),r("h3",{attrs:{id:"wmt-2014-en-fr"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#wmt-2014-en-fr"}},[t._v("#")]),t._v(" WMT 2014 EN-FR")]),t._v(" "),r("p",[t._v("Similarly, models are evaluated on the English-French dataset of the Ninth Workshop on Statistical Machine Translation (WMT 2014) based\non BLEU.")]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("Model")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("BLEU")]),t._v(" "),r("th",[t._v("Paper / Source")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("DeepL")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("45.9")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://www.deepl.com/press.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("DeepL Press release"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Transformer Big + BT (Edunov et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("45.6")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1808.09381.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Understanding Back-Translation at Scale"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("MUSE (Zhao et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("43.5")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1911.09483",target:"_blank",rel:"noopener noreferrer"}},[t._v("MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("DynamicConv (Wu et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("43.2")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1901.10430",target:"_blank",rel:"noopener noreferrer"}},[t._v("Pay Less Attention With Lightweight and Dynamic Convolutions"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Transformer Big (Ott et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("43.2")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1806.00187",target:"_blank",rel:"noopener noreferrer"}},[t._v("Scaling Neural Machine Translation"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("RNMT+ (Chen et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("41.0*")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1804.09849",target:"_blank",rel:"noopener noreferrer"}},[t._v("The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Transformer Big (Vaswani et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("41.0")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1706.03762",target:"_blank",rel:"noopener noreferrer"}},[t._v("Attention Is All You Need"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("MoE (Shazeer et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("40.56")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1701.06538",target:"_blank",rel:"noopener noreferrer"}},[t._v("Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("ConvS2S (Gehring et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("40.46")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1705.03122",target:"_blank",rel:"noopener noreferrer"}},[t._v("Convolutional Sequence to Sequence Learning"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Transformer Base (Vaswani et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("38.1")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1706.03762",target:"_blank",rel:"noopener noreferrer"}},[t._v("Attention Is All You Need"),r("OutboundLink")],1)])])])]),t._v(" "),r("p",[r("RouterLink",{attrs:{to:"/"}},[t._v("Go back to the README")])],1)])}),[],!1,null,null,null);e.default=n.exports}}]);