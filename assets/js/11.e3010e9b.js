(window.webpackJsonp=window.webpackJsonp||[]).push([[11],{253:function(e,t,n){"use strict";n.r(t);var a=n(1),r=Object(a.a)({},(function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("h1",{attrs:{id:"common-sense"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#common-sense"}},[e._v("#")]),e._v(" Common sense")]),e._v(" "),n("p",[e._v('Common sense reasoning tasks are intended to require the model to go beyond pattern\nrecognition. Instead, the model should use "common sense" or world knowledge\nto make inferences.')]),e._v(" "),n("h3",{attrs:{id:"event2mind"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#event2mind"}},[e._v("#")]),e._v(" Event2Mind")]),e._v(" "),n("p",[e._v("Event2Mind is a crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations.\nGiven an event described in a short free-form text, a model should reason about the likely intents and reactions of the\nevent's participants. Models are evaluated based on average cross-entropy (lower is better).")]),e._v(" "),n("table",[n("thead",[n("tr",[n("th",[e._v("Model")]),e._v(" "),n("th",{staticStyle:{"text-align":"center"}},[e._v("Dev")]),e._v(" "),n("th",{staticStyle:{"text-align":"center"}},[e._v("Test")]),e._v(" "),n("th",[e._v("Paper / Source")]),e._v(" "),n("th",[e._v("Code")])])]),e._v(" "),n("tbody",[n("tr",[n("td",[e._v("BiRNN 100d (Rashkin et al., 2018)")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("4.25")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("4.22")]),e._v(" "),n("td",[n("a",{attrs:{href:"https://arxiv.org/abs/1805.06939",target:"_blank",rel:"noopener noreferrer"}},[e._v("Event2Mind: Commonsense Inference on Events, Intents, and Reactions"),n("OutboundLink")],1)]),e._v(" "),n("td")]),e._v(" "),n("tr",[n("td",[e._v("ConvNet (Rashkin et al., 2018)")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("4.44")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("4.40")]),e._v(" "),n("td",[n("a",{attrs:{href:"https://arxiv.org/abs/1805.06939",target:"_blank",rel:"noopener noreferrer"}},[e._v("Event2Mind: Commonsense Inference on Events, Intents, and Reactions"),n("OutboundLink")],1)]),e._v(" "),n("td")])])]),e._v(" "),n("h3",{attrs:{id:"swag"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#swag"}},[e._v("#")]),e._v(" SWAG")]),e._v(" "),n("p",[e._v("Situations with Adversarial Generations (SWAG) is a dataset consisting of 113k multiple\nchoice questions about a rich spectrum of grounded situations.")]),e._v(" "),n("table",[n("thead",[n("tr",[n("th",[e._v("Model")]),e._v(" "),n("th",{staticStyle:{"text-align":"center"}},[e._v("Dev")]),e._v(" "),n("th",{staticStyle:{"text-align":"center"}},[e._v("Test")]),e._v(" "),n("th",[e._v("Paper / Source")]),e._v(" "),n("th",[e._v("Code")])])]),e._v(" "),n("tbody",[n("tr",[n("td",[e._v("BERT Large (Devlin et al., 2018)")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("86.6")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("86.3")]),e._v(" "),n("td",[n("a",{attrs:{href:"https://arxiv.org/abs/1810.04805",target:"_blank",rel:"noopener noreferrer"}},[e._v("BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"),n("OutboundLink")],1)]),e._v(" "),n("td")]),e._v(" "),n("tr",[n("td",[e._v("BERT Base (Devlin et al., 2018)")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("81.6")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("-")]),e._v(" "),n("td",[n("a",{attrs:{href:"https://arxiv.org/abs/1810.04805",target:"_blank",rel:"noopener noreferrer"}},[e._v("BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"),n("OutboundLink")],1)]),e._v(" "),n("td")]),e._v(" "),n("tr",[n("td",[e._v("ESIM + ELMo (Zellers et al., 2018)")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("59.1")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("59.2")]),e._v(" "),n("td",[n("a",{attrs:{href:"http://arxiv.org/abs/1808.05326",target:"_blank",rel:"noopener noreferrer"}},[e._v("SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"),n("OutboundLink")],1)]),e._v(" "),n("td")]),e._v(" "),n("tr",[n("td",[e._v("ESIM + GloVe (Zellers et al., 2018)")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("51.9")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("52.7")]),e._v(" "),n("td",[n("a",{attrs:{href:"http://arxiv.org/abs/1808.05326",target:"_blank",rel:"noopener noreferrer"}},[e._v("SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"),n("OutboundLink")],1)]),e._v(" "),n("td")])])]),e._v(" "),n("h3",{attrs:{id:"winograd-schema-challenge"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#winograd-schema-challenge"}},[e._v("#")]),e._v(" Winograd Schema Challenge")]),e._v(" "),n("p",[e._v("The "),n("a",{attrs:{href:"https://www.aaai.org/ocs/index.php/KR/KR12/paper/view/4492",target:"_blank",rel:"noopener noreferrer"}},[e._v("Winograd Schema Challenge"),n("OutboundLink")],1),e._v("\nis a dataset for common sense reasoning. It employs Winograd Schema questions that\nrequire the resolution of anaphora: the system must identify the antecedent of an ambiguous pronoun in a statement. Models\nare evaluated based on accuracy.")]),e._v(" "),n("p",[e._v("Example:")]),e._v(" "),n("p",[e._v("The trophy doesnâ€™t fit in the suitcase because "),n("em",[e._v("it")]),e._v(" is too big. What is too big?\nAnswer 0: the trophy. Answer 1: the suitcase")]),e._v(" "),n("table",[n("thead",[n("tr",[n("th",[e._v("Model")]),e._v(" "),n("th",{staticStyle:{"text-align":"center"}},[e._v("Score")]),e._v(" "),n("th",[e._v("Paper / Source")]),e._v(" "),n("th",[e._v("Code")])])]),e._v(" "),n("tbody",[n("tr",[n("td",[e._v("Word-LM-partial (Trinh and Le, 2018)")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("62.6")]),e._v(" "),n("td",[n("a",{attrs:{href:"https://arxiv.org/abs/1806.02847",target:"_blank",rel:"noopener noreferrer"}},[e._v("A Simple Method for Commonsense Reasoning"),n("OutboundLink")],1)]),e._v(" "),n("td")]),e._v(" "),n("tr",[n("td",[e._v("Char-LM-partial (Trinh and Le, 2018)")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("57.9")]),e._v(" "),n("td",[n("a",{attrs:{href:"https://arxiv.org/abs/1806.02847",target:"_blank",rel:"noopener noreferrer"}},[e._v("A Simple Method for Commonsense Reasoning"),n("OutboundLink")],1)]),e._v(" "),n("td")]),e._v(" "),n("tr",[n("td",[e._v("USSM + Supervised DeepNet + KB (Liu et al., 2017)")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("52.8")]),e._v(" "),n("td",[n("a",{attrs:{href:"https://aaai.org/ocs/index.php/SSS/SSS17/paper/view/15392",target:"_blank",rel:"noopener noreferrer"}},[e._v("Combing Context and Commonsense Knowledge Through Neural Networks for Solving Winograd Schema Problems"),n("OutboundLink")],1)]),e._v(" "),n("td")])])]),e._v(" "),n("h3",{attrs:{id:"winograd-nli-wnli"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#winograd-nli-wnli"}},[e._v("#")]),e._v(" Winograd NLI (WNLI)")]),e._v(" "),n("p",[e._v("WNLI is a relaxation of the Winograd Schema Challenge proposed as part of the "),n("a",{attrs:{href:"https://arxiv.org/abs/1804.07461",target:"_blank",rel:"noopener noreferrer"}},[e._v("GLUE benchmark"),n("OutboundLink")],1),e._v(" and a conversion to the natural language inference (NLI) format. The task is to predict if the sentence with the pronoun substituted is entailed by the original sentence. While the training set is balanced between two classes (entailment and not entailment), the test set is imbalanced between them (35% entailment, 65% not entailment). The majority baseline is thus 65%, while for the Winograd Schema Challenge it is 50% ("),n("a",{attrs:{href:"https://www.aaai.org/ocs/index.php/SSS/SSS17/paper/view/15392",target:"_blank",rel:"noopener noreferrer"}},[e._v("Liu et al., 2017"),n("OutboundLink")],1),e._v("). The latter is more challenging.")]),e._v(" "),n("p",[e._v("Results are available at the "),n("a",{attrs:{href:"https://gluebenchmark.com/leaderboard",target:"_blank",rel:"noopener noreferrer"}},[e._v("GLUE leaderboard"),n("OutboundLink")],1),e._v(". Here is a subset of results of recent models:")]),e._v(" "),n("table",[n("thead",[n("tr",[n("th",[e._v("Model")]),e._v(" "),n("th",{staticStyle:{"text-align":"center"}},[e._v("Score")]),e._v(" "),n("th",[e._v("Paper / Source")]),e._v(" "),n("th",[e._v("Code")])])]),e._v(" "),n("tbody",[n("tr",[n("td",[e._v("XLNet-Large (ensemble) (Yang et al., 2019)")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("90.4")]),e._v(" "),n("td",[n("a",{attrs:{href:"https://arxiv.org/pdf/1906.08237.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("XLNet: Generalized Autoregressive Pretraining for Language Understanding"),n("OutboundLink")],1)]),e._v(" "),n("td",[n("a",{attrs:{href:"https://github.com/zihangdai/xlnet/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Official"),n("OutboundLink")],1)])]),e._v(" "),n("tr",[n("td",[e._v("MT-DNN-ensemble (Liu et al., 2019)")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("89.0")]),e._v(" "),n("td",[n("a",{attrs:{href:"https://arxiv.org/pdf/1904.09482.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding"),n("OutboundLink")],1)]),e._v(" "),n("td",[n("a",{attrs:{href:"https://github.com/namisan/mt-dnn/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Official"),n("OutboundLink")],1)])]),e._v(" "),n("tr",[n("td",[e._v("Snorkel MeTaL(ensemble) (Ratner et al., 2018)")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("65.1")]),e._v(" "),n("td",[n("a",{attrs:{href:"https://arxiv.org/pdf/1810.02840.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Training Complex Models with Multi-Task Weak Supervision"),n("OutboundLink")],1)]),e._v(" "),n("td",[n("a",{attrs:{href:"https://github.com/HazyResearch/metal",target:"_blank",rel:"noopener noreferrer"}},[e._v("Official"),n("OutboundLink")],1)])])])]),e._v(" "),n("h3",{attrs:{id:"visual-common-sense"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#visual-common-sense"}},[e._v("#")]),e._v(" Visual Common Sense")]),e._v(" "),n("p",[e._v("Visual Commonsense Reasoning (VCR) is a new task and large-scale dataset for cognition-level visual understanding.\nWith one glance at an image, we can effortlessly imagine the world beyond the pixels (e.g. that [person1] ordered\npancakes). While this task is easy for humans, it is tremendously difficult for today's vision systems, requiring\nhigher-order cognition and commonsense reasoning about the world. We formalize this task as Visual Commonsense\nReasoning. In addition to answering challenging visual questions expressed in natural language, a model must provide a\nrationale explaining why its answer is true.")]),e._v(" "),n("table",[n("thead",[n("tr",[n("th",[e._v("Model")]),e._v(" "),n("th",{staticStyle:{"text-align":"center"}},[e._v("Q->A")]),e._v(" "),n("th",{staticStyle:{"text-align":"center"}},[e._v("QA->R")]),e._v(" "),n("th",{staticStyle:{"text-align":"center"}},[e._v("Q->AR")]),e._v(" "),n("th",[e._v("Paper / Source")]),e._v(" "),n("th",[e._v("Code")])])]),e._v(" "),n("tbody",[n("tr",[n("td",[e._v("Human Performance University of Washington (Zellers et al. '18)")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("91.0")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("93.0")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("85.0")]),e._v(" "),n("td",[n("a",{attrs:{href:"https://arxiv.org/abs/1811.10830",target:"_blank",rel:"noopener noreferrer"}},[e._v("From Recognition to Cognition: Visual Commonsense Reasoning"),n("OutboundLink")],1)]),e._v(" "),n("td")]),e._v(" "),n("tr",[n("td",[e._v("Recognition to Cognition Networks University of Washington")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("65.1")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("67.3")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("44.0")]),e._v(" "),n("td",[n("a",{attrs:{href:"https://arxiv.org/abs/1811.10830",target:"_blank",rel:"noopener noreferrer"}},[e._v("From Recognition to Cognition: Visual Commonsense Reasoning"),n("OutboundLink")],1)]),e._v(" "),n("td",[e._v("https://github.com/rowanz/r2c")])]),e._v(" "),n("tr",[n("td",[e._v("BERT-Base Google AI Language (experiment by Rowan)")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("53.9")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("64.5")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("35.0")]),e._v(" "),n("td"),e._v(" "),n("td",[e._v("https://github.com/google-research/bert")])]),e._v(" "),n("tr",[n("td",[e._v("MLB Seoul National University (experiment by Rowan)")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("46.2")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("36.8")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("17.2")]),e._v(" "),n("td"),e._v(" "),n("td",[e._v("https://github.com/jnhwkim/MulLowBiVQA")])]),e._v(" "),n("tr",[n("td",[e._v("Random Performance")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("25.0")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("25.0")]),e._v(" "),n("td",{staticStyle:{"text-align":"center"}},[e._v("6.2")]),e._v(" "),n("td"),e._v(" "),n("td")])])]),e._v(" "),n("h3",{attrs:{id:"record"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#record"}},[e._v("#")]),e._v(" ReCoRD")]),e._v(" "),n("p",[e._v("Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) is a large-scale reading comprehension dataset which requires commonsense reasoning. ReCoRD consists of queries automatically generated from CNN/Daily Mail news articles; the answer to each query is a text span from a summarizing passage of the corresponding news. The goal of ReCoRD is to evaluate a machine's ability of commonsense reasoning in reading comprehension. ReCoRD is pronounced as [ËˆrÉ›kÉ™rd] and is part of the "),n("a",{attrs:{href:"https://arxiv.org/pdf/1905.00537.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("SuperGLUE benchmark"),n("OutboundLink")],1),e._v(".")]),e._v(" "),n("table",[n("thead",[n("tr",[n("th",[e._v("Model")]),e._v(" "),n("th",[e._v("EM")]),e._v(" "),n("th",[e._v("F1")]),e._v(" "),n("th",[e._v("Paper / Source")]),e._v(" "),n("th",[e._v("Code")])])]),e._v(" "),n("tbody",[n("tr",[n("td",[e._v("Human Performance Johns Hopkins University (Zhang et al. '18)")]),e._v(" "),n("td",[e._v("91.31")]),e._v(" "),n("td",[e._v("91.69")]),e._v(" "),n("td",[n("a",{attrs:{href:"https://arxiv.org/pdf/1810.12885.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension"),n("OutboundLink")],1)]),e._v(" "),n("td")]),e._v(" "),n("tr",[n("td",[e._v("RoBERTa (Facebook AI)")]),e._v(" "),n("td",[e._v("90.0")]),e._v(" "),n("td",[e._v("90.6")]),e._v(" "),n("td",[n("a",{attrs:{href:"https://arxiv.org/pdf/1907.11692.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("RoBERTa: A Robustly Optimized BERT Pretraining Approach"),n("OutboundLink")],1)]),e._v(" "),n("td",[n("a",{attrs:{href:"https://github.com/pytorch/fairseq/tree/master/examples/roberta",target:"_blank",rel:"noopener noreferrer"}},[e._v("Official"),n("OutboundLink")],1)])]),e._v(" "),n("tr",[n("td",[e._v("XLNet + MTL + Verifier (ensemble)")]),e._v(" "),n("td",[e._v("83.09")]),e._v(" "),n("td",[e._v("83.74")]),e._v(" "),n("td"),e._v(" "),n("td")]),e._v(" "),n("tr",[n("td",[e._v("CSRLM (single model)")]),e._v(" "),n("td",[e._v("81.78")]),e._v(" "),n("td",[e._v("82.58")]),e._v(" "),n("td"),e._v(" "),n("td")])])])])}),[],!1,null,null,null);t.default=r.exports}}]);