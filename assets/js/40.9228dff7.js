(window.webpackJsonp=window.webpackJsonp||[]).push([[40],{257:function(t,e,r){"use strict";r.r(e);var a=r(1),n=Object(a.a)({},(function(){var t=this,e=t.$createElement,r=t._self._c||e;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h1",{attrs:{id:"summarization"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#summarization"}},[t._v("#")]),t._v(" Summarization")]),t._v(" "),r("p",[t._v("Summarization is the task of producing a shorter version of one or several documents that preserves most of the\ninput's meaning.")]),t._v(" "),r("h3",{attrs:{id:"warning-evaluation-metrics"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#warning-evaluation-metrics"}},[t._v("#")]),t._v(" Warning: Evaluation Metrics")]),t._v(" "),r("p",[t._v("For summarization, automatic metrics such as ROUGE and METEOR have serious limitations:")]),t._v(" "),r("ol",[r("li",[t._v("They only assess content selection and do not account for other quality aspects, such as fluency, grammaticality, coherence, etc.")]),t._v(" "),r("li",[t._v("To assess content selection, they rely mostly on lexical overlap, although an abstractive summary could express they same content as a reference without any lexical overlap.")]),t._v(" "),r("li",[t._v("Given the subjectiveness of summarization and the correspondingly low agreement between annotators, the metrics were designed to be used with multiple reference summaries per input. However, recent datasets such as CNN/DailyMail and Gigaword provide only a single reference.")])]),t._v(" "),r("p",[t._v("Therefore, tracking progress and claiming state-of-the-art based only on these metrics is questionable. Most papers carry out additional manual comparisons of alternative summaries. Unfortunately, such experiments are difficult to compare across papers. If you have an idea on how to do that, feel free to contribute.")]),t._v(" "),r("h3",{attrs:{id:"cnn-daily-mail"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#cnn-daily-mail"}},[t._v("#")]),t._v(" CNN / Daily Mail")]),t._v(" "),r("p",[t._v("The "),r("a",{attrs:{href:"https://arxiv.org/abs/1506.03340",target:"_blank",rel:"noopener noreferrer"}},[t._v("CNN / Daily Mail dataset"),r("OutboundLink")],1),t._v(" as processed by\n"),r("a",{attrs:{href:"http://www.aclweb.org/anthology/K16-1028",target:"_blank",rel:"noopener noreferrer"}},[t._v("Nallapati et al. (2016)"),r("OutboundLink")],1),t._v(" has been used\nfor evaluating summarization. The dataset contains online news articles (781 tokens\non average) paired with multi-sentence summaries (3.75 sentences or 56 tokens on average).\nThe processed version contains 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs.\nModels are evaluated with full-length F1-scores of ROUGE-1, ROUGE-2, ROUGE-L, and METEOR (optional).")]),t._v(" "),r("h4",{attrs:{id:"anonymized-version"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#anonymized-version"}},[t._v("#")]),t._v(" Anonymized version")]),t._v(" "),r("p",[t._v("The following models have been evaluated on the entitiy-anonymized version of the dataset introduced by "),r("a",{attrs:{href:"http://www.aclweb.org/anthology/K16-1028",target:"_blank",rel:"noopener noreferrer"}},[t._v("Nallapati et al. (2016)"),r("OutboundLink")],1),t._v(".")]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("Model")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("ROUGE-1")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("ROUGE-2")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("ROUGE-L")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("METEOR")]),t._v(" "),r("th",[t._v("Paper / Source")]),t._v(" "),r("th",[t._v("Code")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("RNES w/o coherence (Wu and Hu, 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("41.25")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("18.87")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37.75")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16838/16118",target:"_blank",rel:"noopener noreferrer"}},[t._v("Learning to Extract Coherent Summary via Deep Reinforcement Learning"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("SWAP-NET (Jadhav and Rajan, 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("41.6")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("18.3")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37.7")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/P18-1014",target:"_blank",rel:"noopener noreferrer"}},[t._v("Extractive Summarization with SWAP-NET: Sentences and Words from Alternating Pointer Networks"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("HSASS (Al-Sabahi et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("42.3")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("17.8")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37.6")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8344797",target:"_blank",rel:"noopener noreferrer"}},[t._v("A Hierarchical Structured Self-Attentive Model for Extractive Document Summarization (HSSAS)"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("GAN (Liu et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("39.92")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("17.65")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("36.71")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16238/16492",target:"_blank",rel:"noopener noreferrer"}},[t._v("Generative Adversarial Network for Abstractive Text Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("KIGN+Prediction-guide (Li et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("38.95")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("17.12")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("35.68")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/N18-2009",target:"_blank",rel:"noopener noreferrer"}},[t._v("Guiding Generation for Abstractive Text Summarization based on Key Information Guide Network"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("SummaRuNNer (Nallapati et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("39.6")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("16.2")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("35.3")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1611.04230",target:"_blank",rel:"noopener noreferrer"}},[t._v("SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("rnn-ext + abs + RL + rerank (Chen and Bansal, 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("39.66")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("15.85")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37.34")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/P18-1063",target:"_blank",rel:"noopener noreferrer"}},[t._v("Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/ChenRocks/fast_abs_rl",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("ML+RL, with intra-attention (Paulus et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("39.87")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("15.82")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("36.90")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://openreview.net/pdf?id=HkAClQgA-",target:"_blank",rel:"noopener noreferrer"}},[t._v("A Deep Reinforced Model for Abstractive Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("Lead-3 baseline (Nallapati et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("39.2")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("15.7")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("35.5")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1611.04230",target:"_blank",rel:"noopener noreferrer"}},[t._v("SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("ML+RL ROUGE+Novel, with LM (Kryscinski et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("40.02")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("15.53")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37.44")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/D18-1207",target:"_blank",rel:"noopener noreferrer"}},[t._v("Improving Abstraction in Text Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("(Tan et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("38.1")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("13.9")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("34.0")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/P17-1108",target:"_blank",rel:"noopener noreferrer"}},[t._v("Abstractive Document Summarization with a Graph-Based Attentional Neural Model"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("words-lvt2k-temp-att (Nallapti et al., 2016)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("35.46")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("13.30")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("32.65")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://www.aclweb.org/anthology/K16-1028",target:"_blank",rel:"noopener noreferrer"}},[t._v("Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"),r("OutboundLink")],1)]),t._v(" "),r("td")])])]),t._v(" "),r("h4",{attrs:{id:"non-anonymized-version-extractive-models"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#non-anonymized-version-extractive-models"}},[t._v("#")]),t._v(" Non-Anonymized Version: Extractive Models")]),t._v(" "),r("p",[t._v("The following models have been evaluated on the non-anonymized version of the dataset introduced by "),r("a",{attrs:{href:"http://aclweb.org/anthology/P17-1099",target:"_blank",rel:"noopener noreferrer"}},[t._v("See et al. (2017)"),r("OutboundLink")],1),t._v(".")]),t._v(" "),r("p",[t._v("The first table covers Extractive Models, while the second covers abstractive approaches.")]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("Model")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("ROUGE-1")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("ROUGE-2")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("ROUGE-L")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("METEOR")]),t._v(" "),r("th",[t._v("Paper / Source")]),t._v(" "),r("th",[t._v("Code")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("BertSumExt (Liu and Lapata 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("43.85")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("20.34")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("39.90")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1908.08345",target:"_blank",rel:"noopener noreferrer"}},[t._v("Text Summarization with Pretrained Encoders"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/nlpyang/PreSumm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("BERT-ext + RL (Bae et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("42.76")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("19.87")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("39.11")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1909.08752",target:"_blank",rel:"noopener noreferrer"}},[t._v("Summary Level Training of Sentence Rewriting for Abstractive Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("PNBERT (Zhong et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("42.69")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("19.60")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("38.85")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1907.03491",target:"_blank",rel:"noopener noreferrer"}},[t._v("Searching for Effective Neural Extractive Summarization: What Works and What's Next"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/maszhongming/Effective_Extractive_Summarization",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("HIBERT (Zhang et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("42.37")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("19.95")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("38.83")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1905.06566",target:"_blank",rel:"noopener noreferrer"}},[t._v("HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("NeuSUM (Zhou et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("41.59")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("19.01")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37.98")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/P18-1061",target:"_blank",rel:"noopener noreferrer"}},[t._v("Neural Document Summarization by Jointly Learning to Score and Select Sentences"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/magic282/NeuSum",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Latent (Zhang et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("41.05")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("18.77")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37.54")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/D18-1088",target:"_blank",rel:"noopener noreferrer"}},[t._v("Neural Latent Extractive Document Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("BanditSum (Dong et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("41.5")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("18.7")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37.6")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://aclweb.org/anthology/D18-1409",target:"_blank",rel:"noopener noreferrer"}},[t._v("BANDITSUM: Extractive Summarization as a Contextual Bandit"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/yuedongP/BanditSum",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("REFRESH (Narayan et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("40.0")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("18.2")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("36.6")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/N18-1158",target:"_blank",rel:"noopener noreferrer"}},[t._v("Ranking Sentences for Extractive Summarization with Reinforcement Learning"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/EdinburghNLP/Refresh",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Lead-3 baseline (See et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("40.34")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("17.70")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("36.57")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("22.21")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/P17-1099",target:"_blank",rel:"noopener noreferrer"}},[t._v("Get To The Point: Summarization with Pointer-Generator Networks"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/abisee/pointer-generator",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])])])]),t._v(" "),r("h4",{attrs:{id:"non-anonymized-abstractive-models-mixed-models"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#non-anonymized-abstractive-models-mixed-models"}},[t._v("#")]),t._v(" Non-Anonymized: Abstractive Models & Mixed Models")]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("Model")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("ROUGE-1")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("ROUGE-2")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("ROUGE-L")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("METEOR")]),t._v(" "),r("th",[t._v("Paper / Source")]),t._v(" "),r("th",[t._v("Code")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("PEGASUS (Zhang et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("44.17")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("21.47")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("41.11")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1912.08777.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td",[t._v("-")])]),t._v(" "),r("tr",[r("td",[t._v("BART (Lewis et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("44.16")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("21.28")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("40.90")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1910.13461.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/pytorch/fairseq/tree/master/examples/bart",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("ProphetNet (Yan, Qi, Gong, Liu et al., 2020)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("43.68")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("20.64")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("40.72")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/2001.04063.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training"),r("OutboundLink")],1)]),t._v(" "),r("td",[t._v("-")])]),t._v(" "),r("tr",[r("td",[t._v("T5 (Raffel et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("43.52")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("21.55")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("40.69")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1910.10683.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/google-research/text-to-text-transfer-transformer",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("UniLM (Dong et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("43.33")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("20.21")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("40.51")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1905.03197.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Unified Language Model Pre-training for Natural Language Understanding and Generation"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/microsoft/unilm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("CNN-2sent-hieco-RBM (Zhang et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("42.04")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("19.77")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("39.42")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://www.mdpi.com/2076-3417/9/8/1665/pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Abstract Text Summarization with a Convolutional Seq2Seq Model"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("BertSumExtAbs (Liu and Lapata 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("42.13")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("19.60")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("39.18")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1908.08345",target:"_blank",rel:"noopener noreferrer"}},[t._v("Text Summarization with Pretrained Encoders"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/nlpyang/PreSumm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("BERT-ext + abs + RL + rerank (Bae et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("41.90")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("19.08")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("39.64")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1909.08752",target:"_blank",rel:"noopener noreferrer"}},[t._v("Summary Level Training of Sentence Rewriting for Abstractive Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("Two-Stage + RL (Zhang et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("41.71")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("19.49")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("38.79")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1902.09243",target:"_blank",rel:"noopener noreferrer"}},[t._v("Pretraining-Based Natural Language Generation for Text Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("DCA (Celikyilmaz et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("41.69")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("19.47")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37.92")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/N18-1150",target:"_blank",rel:"noopener noreferrer"}},[t._v("Deep Communicating Agents for Abstractive Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("EditNet (Moroshko et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("41.42")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("19.03")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("38.36")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1902.10360",target:"_blank",rel:"noopener noreferrer"}},[t._v("An Editorial Network for Enhanced Document Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("rnn-ext + RL (Chen and Bansal, 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("41.47")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("18.72")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37.76")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("22.35")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/P18-1061",target:"_blank",rel:"noopener noreferrer"}},[t._v("Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/chenrocks/fast_abs_rl",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Bottom-Up Summarization (Gehrmann et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("41.22")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("18.68")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("38.34")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1808.10792",target:"_blank",rel:"noopener noreferrer"}},[t._v("Bottom-Up Abstractive Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/sebastianGehrmann/bottom-up-summary",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("(Li et al., 2018a)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("41.54")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("18.18")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("36.47")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/D18-1205",target:"_blank",rel:"noopener noreferrer"}},[t._v("Improving Neural Abstractive Document Summarization with Explicit Information Selection Modeling"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("(Li et al., 2018b)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("40.30")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("18.02")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37.36")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/D18-1441",target:"_blank",rel:"noopener noreferrer"}},[t._v("Improving Neural Abstractive Document Summarization with Structural Regularization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("ROUGESal+Ent RL (Pasunuru and Bansal, 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("40.43")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("18.00")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37.10")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("20.02")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/N18-2102",target:"_blank",rel:"noopener noreferrer"}},[t._v("Multi-Reward Reinforced Summarization with Saliency and Entailment"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("end2end w/ inconsistency loss (Hsu et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("40.68")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("17.97")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37.13")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/P18-1013",target:"_blank",rel:"noopener noreferrer"}},[t._v("A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("RL + pg + cbdec (Jiang and Bansal, 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("40.66")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("17.87")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37.06")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("20.51")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/D18-1440",target:"_blank",rel:"noopener noreferrer"}},[t._v("Closed-Book Training to Improve Summarization Encoder Memory"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("rnn-ext + abs + RL + rerank (Chen and Bansal, 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("40.88")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("17.80")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("38.54")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("20.38")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/P18-1061",target:"_blank",rel:"noopener noreferrer"}},[t._v("Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/chenrocks/fast_abs_rl",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Pointer + Coverage + EntailmentGen + QuestionGen (Guo et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("39.81")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("17.64")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("36.54")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("18.54")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/P18-1064",target:"_blank",rel:"noopener noreferrer"}},[t._v("Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("ML+RL ROUGE+Novel, with LM (Kryscinski et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("40.19")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("17.38")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37.52")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("-")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/D18-1207",target:"_blank",rel:"noopener noreferrer"}},[t._v("Improving Abstraction in Text Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("Pointer-generator + coverage (See et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("39.53")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("17.28")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("36.38")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("18.72")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/P17-1099",target:"_blank",rel:"noopener noreferrer"}},[t._v("Get To The Point: Summarization with Pointer-Generator Networks"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/abisee/pointer-generator",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])])])]),t._v(" "),r("h3",{attrs:{id:"gigaword"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#gigaword"}},[t._v("#")]),t._v(" Gigaword")]),t._v(" "),r("p",[t._v("The Gigaword summarization dataset has been first used by "),r("a",{attrs:{href:"https://www.aclweb.org/anthology/D/D15/D15-1044.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Rush et al., 2015"),r("OutboundLink")],1),t._v(" and represents a sentence summarization / headline generation task with very short input documents (31.4 tokens) and summaries (8.3 tokens). It contains 3.8M training, 189k development and 1951 test instances. Models are evaluated with ROUGE-1, ROUGE-2 and ROUGE-L using full-length F1-scores.")]),t._v(" "),r("p",[t._v("Below Results are ranking by ROUGE-2 Scores.")]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("Model")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("ROUGE-1")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("ROUGE-2*")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("ROUGE-L")]),t._v(" "),r("th",[t._v("Paper / Source")]),t._v(" "),r("th",[t._v("Code")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("ControlCopying (Song et al., 2020)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("39.08")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("20.47")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("36.69")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1911.10390.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Controlling the Amount of Verbatim Copying in Abstractive Summarizatio"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/ucfnlp/control-over-copying",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("ProphetNet (Yan, Qi, Gong, Liu et al., 2020)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("39.23")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("20.36")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("36.57")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/2001.04063.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training"),r("OutboundLink")],1)]),t._v(" "),r("td",[t._v("-")])]),t._v(" "),r("tr",[r("td",[t._v("UniLM (Dong et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("38.90")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("20.05")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("36.00")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1905.03197.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Unified Language Model Pre-training for Natural Language Understanding and Generation"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/microsoft/unilm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("PEGASUS (Zhang et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("39.12")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("19.86")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("36.24")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1912.08777.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td",[t._v("-")])]),t._v(" "),r("tr",[r("td",[t._v("BiSET (Wang et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("39.11")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("19.78")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("36.87")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://www.aclweb.org/anthology/P19-1207",target:"_blank",rel:"noopener noreferrer"}},[t._v("BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/InitialBug/BiSET",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("MASS (Song et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("38.73")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("19.71")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("35.96")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1905.02450v5.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("MASS: Masked Sequence to Sequence Pre-training for Language Generation"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/microsoft/MASS",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Re^3 Sum (Cao et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37.04")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("19.03")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("34.46")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/P18-1015",target:"_blank",rel:"noopener noreferrer"}},[t._v("Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("JointParsing (Song el at., 2020)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("36.61")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("18.85")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("34.33")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1911.10389.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Joint Parsing and Generation for Abstractive Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/KaiQiangSong/joint_parse_summ",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("CNN-2sent-hieco-RBM (Zhang et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37.95")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("18.64")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("35.11")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://www.mdpi.com/2076-3417/9/8/1665/pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Abstract Text Summarization with a Convolutional Seq2Seq Model"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("Reinforced-Topic-ConvS2S (Wang et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("36.92")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("18.29")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("34.58")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://www.ijcai.org/proceedings/2018/0619.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("CGU (Lin et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("36.3")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("18.0")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("33.8")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/P18-2027",target:"_blank",rel:"noopener noreferrer"}},[t._v("Global Encoding for Abstractive Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://www.github.com/lancopku/Global-Encoding",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Pointer + Coverage + EntailmentGen + QuestionGen (Guo et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("35.98")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("17.76")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("33.63")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/P18-1064",target:"_blank",rel:"noopener noreferrer"}},[t._v("Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("Struct+2Way+Word (Song et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("35.47")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("17.66")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("33.52")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/C18-1146",target:"_blank",rel:"noopener noreferrer"}},[t._v("Structure-Infused Copy Mechanisms for Abstractive Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/KaiQiangSong/struct_infused_summ",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("FTSum_g (Cao et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37.27")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("17.65")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("34.24")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1711.04434.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Faithful to the Original: Fact Aware Neural Abstractive Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("DRGD (Li et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("36.27")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("17.57")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("33.62")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/D17-1222",target:"_blank",rel:"noopener noreferrer"}},[t._v("Deep Recurrent Generative Decoder for Abstractive Text Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("SEASS (Zhou et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("36.15")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("17.54")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("33.63")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/P17-1101",target:"_blank",rel:"noopener noreferrer"}},[t._v("Selective Encoding for Abstractive Sentence Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/magic282/SEASS",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("EndDec+WFE (Suzuki and Nagata, 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("36.30")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("17.31")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("33.88")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/E17-2047",target:"_blank",rel:"noopener noreferrer"}},[t._v("Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("Seq2seq + selective + MTL + ERAM (Li et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("35.33")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("17.27")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("33.19")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/C18-1121",target:"_blank",rel:"noopener noreferrer"}},[t._v("Ensure the Correctness of the Summary: Incorporate Entailment Knowledge into Abstractive Sentence Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("Seq2seq + E2T_cnn (Amplayo et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37.04")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("16.66")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("34.93")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/N18-1064",target:"_blank",rel:"noopener noreferrer"}},[t._v("Entity Commonsense Representation for Neural Abstractive Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("RAS-Elman (Chopra et al., 2016)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("33.78")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("15.97")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("31.15")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://www.aclweb.org/anthology/N16-1012",target:"_blank",rel:"noopener noreferrer"}},[t._v("Abstractive Sentence Summarization with Attentive Recurrent Neural Networks"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("words-lvt5k-1sent (Nallapti et al., 2016)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("32.67")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("15.59")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("30.64")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://www.aclweb.org/anthology/K16-1028",target:"_blank",rel:"noopener noreferrer"}},[t._v("Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("ABS+ (Rush et al., 2015)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("29.76")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("11.88")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("26.96")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://www.aclweb.org/anthology/D/D15/D15-1044.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("A Neural Attention Model for Sentence Summarization"),r("OutboundLink")],1),t._v(" *")]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("ABS (Rush et al., 2015)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("29.55")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("11.32")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("26.42")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://www.aclweb.org/anthology/D/D15/D15-1044.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("A Neural Attention Model for Sentence Summarization"),r("OutboundLink")],1),t._v(" *")]),t._v(" "),r("td")])])]),t._v(" "),r("p",[t._v("(*) "),r("a",{attrs:{href:"https://www.aclweb.org/anthology/D/D15/D15-1044.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Rush et al., 2015"),r("OutboundLink")],1),t._v("  report ROUGE recall, the table here contains ROUGE F1-scores for Rush's model reported by "),r("a",{attrs:{href:"http://www.aclweb.org/anthology/N16-1012",target:"_blank",rel:"noopener noreferrer"}},[t._v("Chopra et al., 2016"),r("OutboundLink")],1)]),t._v(" "),r("h3",{attrs:{id:"x-sum"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#x-sum"}},[t._v("#")]),t._v(" X-Sum")]),t._v(" "),r("p",[t._v("X-Sum (standing for "),r("em",[t._v("Extreme Summarization")]),t._v("), introduced by "),r("a",{attrs:{href:"https://arxiv.org/pdf/1808.08745.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Narayan et al., 2018"),r("OutboundLink")],1),t._v(", is a summarization dataset which does not favor extractive strategies and calls for an abstractive modeling approach."),r("br"),t._v("\nThe idea of this dataset is to create a short, one sentence news summary."),r("br"),t._v("\nData is collected by harvesting online articles from the BBC."),r("br"),t._v("\nThe dataset contain "),r("strong",[t._v("204 045")]),t._v(" samples for the training set, "),r("strong",[t._v("11 332")]),t._v(" for the validation set, and "),r("strong",[t._v("11 334")]),t._v(" for the test set. In average the length of article is 431 words (~20 sentences) and the length of summary is 23 words. It can be downloaded "),r("a",{attrs:{href:"https://github.com/EdinburghNLP/XSum",target:"_blank",rel:"noopener noreferrer"}},[t._v("here"),r("OutboundLink")],1),t._v("."),r("br"),t._v("\nEvaluation metrics are ROUGE-1, ROUGE-2 and ROUGE-L.")]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("Model")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("ROUGE-1")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("ROUGE-2")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("ROUGE-L")]),t._v(" "),r("th",[t._v("Paper / Source")]),t._v(" "),r("th",[t._v("Code")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("PEGASUS (Zhang et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("47.21")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("24.56")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("39.25")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1912.08777.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td",[t._v("-")])]),t._v(" "),r("tr",[r("td",[t._v("BART (Lewis et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("45.14")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("22.27")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("37.25")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1910.13461.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/pytorch/fairseq/tree/master/examples/bart",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("BertSumExtAbs (Liu et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("38.81")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("16.50")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("31.27")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1908.08345.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Text Summarization with Pretrained Encoders"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/nlpyang/PreSumm",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("T-ConvS2S")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("31.89")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("11.54")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("25.75")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1808.08745.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Don’t Give Me the Details, Just the Summary!"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/EdinburghNLP/XSum",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("PtGen")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("29.70")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("9.21")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("23.24")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1808.08745.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Don’t Give Me the Details, Just the Summary!"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/EdinburghNLP/XSum",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Seq2Seq")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("28.42")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("8.77")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("22.48")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1808.08745.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Don’t Give Me the Details, Just the Summary!"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/EdinburghNLP/XSum",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("PtGen-Covg")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("28.10")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("8.02")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("21.72")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1808.08745.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Don’t Give Me the Details, Just the Summary!"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/EdinburghNLP/XSum",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Baseline : Extractive Oracle")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("29.79")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("8.81")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("22.66")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1808.08745.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Don’t Give Me the Details, Just the Summary!"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/EdinburghNLP/XSum",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Baseline : Lead-3")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("16.30")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.60")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("11.95")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1808.08745.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Don’t Give Me the Details, Just the Summary!"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/EdinburghNLP/XSum",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("Baseline : Random")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("15.16")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.78")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("11.27")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/pdf/1808.08745.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Don’t Give Me the Details, Just the Summary!"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/EdinburghNLP/XSum",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])])])]),t._v(" "),r("h3",{attrs:{id:"duc-2004-task-1"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#duc-2004-task-1"}},[t._v("#")]),t._v(" DUC 2004 Task 1")]),t._v(" "),r("p",[t._v("Similar to Gigaword, task 1 of "),r("a",{attrs:{href:"https://duc.nist.gov/duc2004/",target:"_blank",rel:"noopener noreferrer"}},[t._v("DUC 2004"),r("OutboundLink")],1),t._v(" is a sentence summarization task. The dataset contains 500 documents with on average 35.6 tokens and summaries with 10.4 tokens. Due to its size, neural models are typically trained on other datasets and only tested on DUC 2004. Evaluation metrics are ROUGE-1, ROUGE-2 and ROUGE-L recall @ 75 bytes.")]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("Model")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("ROUGE-1")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("ROUGE-2")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("ROUGE-L")]),t._v(" "),r("th",[t._v("Paper / Source")]),t._v(" "),r("th",[t._v("Code")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("Transformer + LRPE + PE + Re-ranking (Takase and Okazaki, 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("32.29")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("11.49")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("28.03")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://arxiv.org/abs/1904.07418",target:"_blank",rel:"noopener noreferrer"}},[t._v("Positional Encoding to Control Output Sequence Length"),r("OutboundLink")],1)]),t._v(" "),r("td",[r("a",{attrs:{href:"https://github.com/takase/control-length",target:"_blank",rel:"noopener noreferrer"}},[t._v("Official"),r("OutboundLink")],1)])]),t._v(" "),r("tr",[r("td",[t._v("DRGD (Li et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("31.79")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("10.75")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("27.48")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/D17-1222",target:"_blank",rel:"noopener noreferrer"}},[t._v("Deep Recurrent Generative Decoder for Abstractive Text Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("EndDec+WFE (Suzuki and Nagata, 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("32.28")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("10.54")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("27.8")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/E17-2047",target:"_blank",rel:"noopener noreferrer"}},[t._v("Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("Reinforced-Topic-ConvS2S (Wang et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("31.15")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("10.85")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("27.68")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://www.ijcai.org/proceedings/2018/0619.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("CNN-2sent-hieco-RBM (Zhang et al., 2019)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("29.74")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("9.85")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("25.81")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://www.mdpi.com/2076-3417/9/8/1665/pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Abstract Text Summarization with a Convolutional Seq2Seq Model"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("Seq2seq + selective + MTL + ERAM (Li et al., 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("29.33")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("10.24")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("25.24")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/C18-1121",target:"_blank",rel:"noopener noreferrer"}},[t._v("Ensure the Correctness of the Summary: Incorporate Entailment Knowledge into Abstractive Sentence Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("SEASS (Zhou et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("29.21")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("9.56")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("25.51")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://aclweb.org/anthology/P17-1101",target:"_blank",rel:"noopener noreferrer"}},[t._v("Selective Encoding for Abstractive Sentence Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("words-lvt5k-1sent (Nallapti et al., 2016)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("28.61")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("9.42")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("25.24")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://www.aclweb.org/anthology/K16-1028",target:"_blank",rel:"noopener noreferrer"}},[t._v("Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("ABS+ (Rush et al., 2015)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("28.18")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("8.49")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("23.81")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://www.aclweb.org/anthology/D/D15/D15-1044.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("A Neural Attention Model for Sentence Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("RAS-Elman (Chopra et al., 2016)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("28.97")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("8.26")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("24.06")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://www.aclweb.org/anthology/N16-1012",target:"_blank",rel:"noopener noreferrer"}},[t._v("Abstractive Sentence Summarization with Attentive Recurrent Neural Networks"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("ABS (Rush et al., 2015)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("26.55")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("7.06")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("22.05")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://www.aclweb.org/anthology/D/D15/D15-1044.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("A Neural Attention Model for Sentence Summarization"),r("OutboundLink")],1)]),t._v(" "),r("td")])])]),t._v(" "),r("h2",{attrs:{id:"webis-tldr-17-corpus"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#webis-tldr-17-corpus"}},[t._v("#")]),t._v(" Webis-TLDR-17 Corpus")]),t._v(" "),r("p",[t._v("This "),r("a",{attrs:{href:"https://zenodo.org/record/1168855",target:"_blank",rel:"noopener noreferrer"}},[t._v("dataset"),r("OutboundLink")],1),t._v(" contains 3 Million pairs of content and self-written summaries mined from Reddit. It is one of the first large-scale summarization dataset from the social media domain. For more details, refer to "),r("a",{attrs:{href:"https://aclweb.org/anthology/W17-4508",target:"_blank",rel:"noopener noreferrer"}},[t._v("TL;DR: Mining Reddit to Learn Automatic Summarization"),r("OutboundLink")],1)]),t._v(" "),r("h2",{attrs:{id:"sentence-compression"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#sentence-compression"}},[t._v("#")]),t._v(" Sentence Compression")]),t._v(" "),r("p",[t._v("Sentence compression produces a shorter sentence by removing redundant information,\npreserving the grammatically and the important content of the original sentence.")]),t._v(" "),r("h3",{attrs:{id:"google-dataset"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#google-dataset"}},[t._v("#")]),t._v(" Google Dataset")]),t._v(" "),r("p",[t._v("The "),r("a",{attrs:{href:"https://github.com/google-research-datasets/sentence-compression",target:"_blank",rel:"noopener noreferrer"}},[t._v("Google Dataset"),r("OutboundLink")],1),t._v(" was built by Filippova et al., 2013("),r("a",{attrs:{href:"https://www.aclweb.org/anthology/D/D13/D13-1155.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Overcoming the Lack of Parallel Data in Sentence Compression"),r("OutboundLink")],1),t._v("). The first dataset released contained only 10,000 sentence-compression pairs, but last year was released an additional 200,000 pairs.")]),t._v(" "),r("p",[t._v("Example of a sentence-compression pair:")]),t._v(" "),r("blockquote",[r("p",[t._v("Sentence: Floyd Mayweather is open to fighting Amir Khan in the future, despite snubbing the Bolton-born boxer in favour of a May bout with Argentine Marcos Maidana, according to promoters Golden Boy")])]),t._v(" "),r("blockquote",[r("p",[t._v("Compression: Floyd Mayweather is open to fighting Amir Khan in the future.")])]),t._v(" "),r("p",[t._v("In short, this is a deletion-based task where the compression is a subsequence from the original sentence. From the 10,000 pairs of the eval portion("),r("a",{attrs:{href:"https://github.com/google-research-datasets/sentence-compression/tree/master/data",target:"_blank",rel:"noopener noreferrer"}},[t._v("repository"),r("OutboundLink")],1),t._v(") it is used the very first 1,000 sentence for automatic evaluation and the 200,000 pairs for training.")]),t._v(" "),r("p",[t._v("Models are evaluated using the following metrics:")]),t._v(" "),r("ul",[r("li",[t._v("F1 - compute the recall and precision in terms of tokens kept in the golden and the generated compressions.")]),t._v(" "),r("li",[t._v("Compression rate (CR) - the length of the compression in characters divided over the sentence length.")])]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("Model")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("F1")]),t._v(" "),r("th",[t._v("CR")]),t._v(" "),r("th",[t._v("Paper / Source")]),t._v(" "),r("th",[t._v("Code")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("BiRNN + LM Evaluator (Zhao et al. 2018)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("0.851")]),t._v(" "),r("td",[t._v("0.39")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://aclweb.org/anthology/P18-2028",target:"_blank",rel:"noopener noreferrer"}},[t._v("A Language Model based Evaluator for Sentence Compression"),r("OutboundLink")],1)]),t._v(" "),r("td",[t._v("https://github.com/code4conference/code4sc")])]),t._v(" "),r("tr",[r("td",[t._v("LSTM (Filippova et al., 2015)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("0.82")]),t._v(" "),r("td",[t._v("0.38")]),t._v(" "),r("td",[r("a",{attrs:{href:"https://research.google.com/pubs/archive/43852.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Sentence Compression by Deletion with LSTMs"),r("OutboundLink")],1)]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("BiLSTM (Wang et al., 2017)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("0.8")]),t._v(" "),r("td",[t._v("0.43")]),t._v(" "),r("td",[r("a",{attrs:{href:"http://www.aclweb.org/anthology/P17-1127",target:"_blank",rel:"noopener noreferrer"}},[t._v("Can Syntax Help? Improving an LSTM-based Sentence Compression Model for New Domains"),r("OutboundLink")],1)]),t._v(" "),r("td")])])]),t._v(" "),r("p",[r("RouterLink",{attrs:{to:"/"}},[t._v("Go back to the README")])],1)])}),[],!1,null,null,null);e.default=n.exports}}]);